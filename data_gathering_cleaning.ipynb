{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Gathering and Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Energy data for this project is gathered from the U.S. Energy Information Administration (EIA) [website](https://www.eia.gov).\n",
    "\n",
    "Weather data for this project is gathered from the National Centers for Environmental Information. Documentation for the API can be found [here](https://www.ncei.noaa.gov/support/access-data-service-api-user-documentation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import requests\n",
    "import time\n",
    "from bs4 import BeautifulSoup as BS\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import helper_functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to do a little manipulation to the text file from EIA. It is a text file with a bunch of line-separated JSON objects, but I massage it here to a proper JSON and export it as a new file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "lastline = None\n",
    "\n",
    "# Open and read text file\n",
    "with open(\"SEDS.txt\",\"r\") as f:\n",
    "    lineList = f.readlines()\n",
    "    \n",
    "    # Keep track of last line\n",
    "    lastline=lineList[-1]\n",
    "\n",
    "# Open text file and create new json to be written\n",
    "with open(\"SEDS.txt\",\"r\") as f, open(\"cleanfile.json\",\"w\") as g:\n",
    "    \n",
    "    # Iterate through each line of the text file\n",
    "    for i,line in enumerate(f,0):\n",
    "        \n",
    "        # First line gets [ and , to initialize the json\n",
    "        if i == 0:\n",
    "            line = \"[\"+str(line)+\",\"\n",
    "            g.write(line)\n",
    "            \n",
    "        # Last line gets ] to signal the end of the json\n",
    "        elif line == lastline:            \n",
    "            g.write(line)\n",
    "            g.write(\"]\")\n",
    "            \n",
    "        # Other lines get comma separation\n",
    "        else:\n",
    "            line = str(line)+\",\"\n",
    "            g.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('cleanfile.json', 'r')\n",
    "json_data = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following energy types were selected based on the categories in the [EIA educational page](https://www.eia.gov/energyexplained/energy-and-the-environment/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "energy_types = [\n",
    "    'All Petroleum Products excluding Fuel Ethanol',\n",
    "    'Coal',\n",
    "    'Natural Gas including Supplemental Gaseous Fuels',\n",
    "    'Nuclear Power',\n",
    "    'Biomass',\n",
    "    'Fuel Ethanol excluding Denaturant',\n",
    "    'Geothermal',\n",
    "    'Hydroelectricity',\n",
    "    'Solar Energy',\n",
    "    'Wind Energy',\n",
    "    'Renewable Energy'\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "# Make all lowercase in case some pages have inconsistent letter casing\n",
    "for i in range(len(energy_types)):\n",
    "    energy_types[i] = energy_types[i].lower()\n",
    "    \n",
    "nonrenewable_energies = energy_types[:4]\n",
    "\n",
    "renewable_energies = energy_types[4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['all petroleum products excluding fuel ethanol',\n",
       " 'coal',\n",
       " 'natural gas including supplemental gaseous fuels',\n",
       " 'nuclear power',\n",
       " 'biomass',\n",
       " 'fuel ethanol excluding denaturant',\n",
       " 'geothermal',\n",
       " 'hydroelectricity',\n",
       " 'solar energy',\n",
       " 'wind energy',\n",
       " 'renewable energy']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "energy_types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web scrape to get the information for each of the above types of energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {'user-agent': 'Safari/13.0.2 (Macintosh; Intel Mac OS X 10_15)'}\n",
    "base_url = 'https://www.eia.gov/opendata/qb.php'\n",
    "consumption_suffix = '?category=40204'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "consumption_page = helper_functions.get_page(base_url+consumption_suffix,headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty dict to store all info across every sector and energy type by state\n",
    "env_series_ids = {}\n",
    "\n",
    "# Start by scraping the consumption website in order to get the list of available sectors  \n",
    "consumption_sectors = consumption_page.find('div',{'class':'pagecontent mr_temp2'})\n",
    "\n",
    "# Store sector url suffixes in a list\n",
    "sector_url_suffixes = [sector.a['href'] for sector in consumption_sectors.find_all('li')[:7]]\n",
    "\n",
    "# Loop 1 - iterate through each sector\n",
    "for sector_url_suffix in sector_url_suffixes:\n",
    "    \n",
    "    # Scrape the sector page\n",
    "    sector_page = helper_functions.get_page(base_url+sector_url_suffix,headers)    \n",
    "\n",
    "    # Go into first url and grab tags of all children categories\n",
    "    children_categories = sector_page.find('section').ul.find_all('li')\n",
    "\n",
    "    # Store the urls of children cats (ccats = children categories)\n",
    "    ccats_url_suffixes = [children_category.a['href'] \n",
    "                          for children_category in children_categories\n",
    "                          if children_category.text.lower() in energy_types]\n",
    "    \n",
    "    # Loop 2 - for each sector, iterate through the relevant types of energy consumption to get state-level data\n",
    "    for ccats_url_suffix in ccats_url_suffixes:\n",
    "        \n",
    "        # Scrape the child category page\n",
    "        child_category_page = helper_functions.get_page(base_url+ccats_url_suffix,headers)\n",
    "\n",
    "        # Grab tags of all energy unit children categories. Only want Btu\n",
    "        energy_unit_cats = child_category_page.find('div',{'class':'main_col'}).ul.find_all('li')\n",
    "\n",
    "        # Store only the url of the 'Btu' children category. I make a list and select only the first element \n",
    "        # because sometimes there will be two energy unit options or just one. This way ensures we only take \n",
    "        # the Btu option.\n",
    "        btu_url_suffix = [energy_unit.a['href'] \n",
    "                   for energy_unit in energy_unit_cats\n",
    "                   if energy_unit.text == 'Btu'][0]\n",
    "        \n",
    "        # Scrape the Btu page\n",
    "        btu_page = helper_functions.get_page(base_url+btu_url_suffix,headers)\n",
    "        \n",
    "        # Get list of states by their tags\n",
    "        states = btu_page.find('div',{'class':'main_col'}).ul.find_all('li')\n",
    "        \n",
    "        # Get url suffixes for each state\n",
    "        state_url_suffixes = [state.a['href'] for state in states]\n",
    "        \n",
    "        # Isolate the sector and energy type\n",
    "        sector = btu_page.find('div',{'class':'main_col'}).h3.find_all('a')[3].text\n",
    "        energy_type = btu_page.find('div',{'class':'main_col'}).h3.find_all('a')[4].text\n",
    "        \n",
    "        # Add these to a dict which will be the values of the overarching env_series_ids dict\n",
    "        series_id_values = {'sector':sector,'energy_type':energy_type}\n",
    "        \n",
    "        # Parse through url suffixes to get and store the series ids we want to use to parse the big JSON\n",
    "        for state_suffix in state_url_suffixes:\n",
    "            series_id = re.findall('SEDS.*',state_suffix)[0]\n",
    "            env_series_ids[series_id] = series_id_values\n",
    "              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse energy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up empty bucket for parsed data\n",
    "environmental_data = []\n",
    "\n",
    "# Iterate through big json to parse relevant info\n",
    "for single_json in json_data:\n",
    "    \n",
    "    # Only parse entries that have the series ids that we care about\n",
    "    if single_json.get('series_id') in env_series_ids.keys():\n",
    "        single_data_entry = {}\n",
    "        single_data_entry['series_id'] = single_json['series_id']\n",
    "        single_data_entry['sector'] = env_series_ids[single_json['series_id']]['sector']\n",
    "        single_data_entry['data'] = single_json['data']\n",
    "        single_data_entry['state'] = re.findall('(, )(\\w* ?\\w* ?\\w*)',single_json['name'])[-1][-1]\n",
    "        single_data_entry['units'] = single_json['units']\n",
    "        single_data_entry['energy_type'] = env_series_ids[single_json['series_id']]['energy_type']\n",
    "        # Add a column for whether or not this type of energy is renewable\n",
    "        if single_data_entry['energy_type'] in renewable_energies:\n",
    "            single_data_entry['renewable'] = 1\n",
    "        else: \n",
    "            single_data_entry['renewable'] = 0\n",
    "        environmental_data.append(single_data_entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "sectors = [data['sector'] for data in environmental_data]\n",
    "sectors = list(set(sectors))\n",
    "sectors\n",
    "del sector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web scrape for population and GDP data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "population_url_suffix = '?category=40367'\n",
    "gdp_url_suffix = '?category=40828'\n",
    "pop_gdp_url_suffixes = [population_url_suffix, gdp_url_suffix]\n",
    "\n",
    "# Create container for  data\n",
    "pop_gdp_series_ids = {}\n",
    "\n",
    "for pop_gdp_url_suffix in pop_gdp_url_suffixes:\n",
    "    \n",
    "    # Scrape population page\n",
    "    page = helper_functions.get_page(base_url + pop_gdp_url_suffix,headers)\n",
    "\n",
    "    # Isolate html tags containing urls for each state\n",
    "    state_tags = page.find('div',{'class':'main_col'}).ul.find_all('li')\n",
    "\n",
    "    # Extract and save each state url suffix\n",
    "    state_url_suffixes = [state_tag.a['href'] \n",
    "                          for state_tag in state_tags]\n",
    "\n",
    "    # Iterate through each state url suffix to extract features\n",
    "    for state_url_suffix in state_url_suffixes:\n",
    "\n",
    "        # Scrape each state's series page\n",
    "        state_page = helper_functions.get_page(base_url + state_url_suffix,headers)\n",
    "\n",
    "        # Isolate html tags containing state name, get text from tag, parse for name\n",
    "        api_call_tags = state_page.find('div',{'class':'main_col'}).find('div',{'class':'api_call_container'})\n",
    "        state_text = api_call_tags.find_all('p')[1].text\n",
    "        state = re.findall('(, )(.*)',state_text)[0][1]\n",
    "        \n",
    "        # Isolate html tags containing description (gdp or pop), get text from tag\n",
    "        main_col_tags = state_page.find('div',{'class':'main_col'}).h3\n",
    "        desc = main_col_tags.find_all('a')[2].text\n",
    "        \n",
    "        # Parse url suffix for series id\n",
    "        series_id = re.findall('SEDS.*',state_url_suffix)[0]\n",
    "\n",
    "        # Add to data container\n",
    "        values = {'state':state,'description':desc}\n",
    "        pop_gdp_series_ids[series_id] = values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse population and gdp data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up empty bucket for parsed data\n",
    "pop_gdp_data = []\n",
    "\n",
    "# Iterate through big json to parse relevant info\n",
    "for single_json in json_data:\n",
    "    \n",
    "    # Only parse entries that have the series ids that we care about\n",
    "    if single_json.get('series_id') in pop_gdp_series_ids.keys():\n",
    "        single_data_entry = {}\n",
    "        single_data_entry['series_id'] = single_json['series_id']\n",
    "        single_data_entry['description'] = pop_gdp_series_ids[single_json['series_id']]['description']\n",
    "        single_data_entry['units'] = single_json['units']\n",
    "        single_data_entry['data'] = single_json['data']\n",
    "        single_data_entry['state'] = pop_gdp_series_ids[single_json['series_id']]['state']\n",
    "        pop_gdp_data.append(single_data_entry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Weather data from NCEI \n",
    "National Centeres for Environmental Information has an Access Data Service that provides a RESTful API. From it, I gather weather information from 10 random cities per state to generate mean max and min temperatures of a given state on any day.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load GDCND weather station codes\n",
    "\n",
    "This is how we tell the API where we want to retrieve data from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of GHCND station codes\n",
    "with open(\"ghcnd.txt\",\"r\") as f:\n",
    "    lineList = f.readlines()\n",
    "\n",
    "# Turn each line into a list for easier sorting\n",
    "split_lines = [line.split() for line in lineList]\n",
    "\n",
    "# The city names got split too, so I rejoin them back together\n",
    "for i in range(len(split_lines)):\n",
    "    if len(split_lines[i]) > 6:\n",
    "        split_lines[i] = split_lines[i][:5] + [' '.join(split_lines[i][5:])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['USC00010008', '31.5703', '-85.2483', '139.0', 'AL', 'ABBEVILLE']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_lines[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load state names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['AL', 'AK', 'AZ', 'AR', 'CA', 'CO', 'CT', 'DE', 'FL', 'GA', 'HI', 'ID', 'IL', 'IN', 'IA', 'KS', 'KY', 'LA', 'ME', 'MD', 'MA', 'MI', 'MN', 'MS', 'MO', 'MT', 'NE', 'NV', 'NH', 'NJ', 'NM', 'NY', 'NC', 'ND', 'OH', 'OK', 'OR', 'PA', 'RI', 'SC', 'SD', 'TN', 'TX', 'UT', 'VT', 'VA', 'WA', 'WV', 'WI', 'WY'])"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the state capitals, convert to dict\n",
    "import csv\n",
    "state_capitals = open('us-state-capitals.csv')\n",
    "state_capitals_reader = csv.reader(state_capitals)\n",
    "state_capitals_dict = dict(state_capitals_reader)\n",
    "state_capitals_dict.pop('')\n",
    "state_capitals_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find indexes where each state starts and ends in the GHCND list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty dict to store this info\n",
    "state_indexes = {}\n",
    "\n",
    "# Iterate through state abbreviations\n",
    "for state in state_capitals_dict:\n",
    "    \n",
    "    # Create a flag to keep track of indexes where a state starts and ends in split_lines\n",
    "    flag = False\n",
    "    \n",
    "    # Create start and end index variables\n",
    "    start_index = None\n",
    "    end_index = None\n",
    "    \n",
    "    # Iterate through station lines\n",
    "    for index in range(len(split_lines)):\n",
    "        \n",
    "        # record index of only the first occurence where we find the state code\n",
    "        if split_lines[index][4] == state and not flag:\n",
    "            start_index = index\n",
    "            \n",
    "            # Switch flag to satisfy the next conditional\n",
    "            flag = True\n",
    "        \n",
    "        # record index of only the first occurence where we don't find the state code AFTER having found it\n",
    "        if split_lines[index][4] != state and flag:\n",
    "            end_index = index\n",
    "            \n",
    "            # end this loop\n",
    "            break\n",
    "            \n",
    "    # Store the indexes        \n",
    "    state_indexes[state] = [start_index, end_index]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For one state, generate a random city code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GA\n",
      "max number of iterations is 367\n",
      "have made 0 api calls\n",
      "have made 0 api calls\n",
      "have made 0 api calls\n",
      "have made 0 api calls\n",
      "just finished GA. 43 states remaining\n",
      "HI\n",
      "max number of iterations is 587\n",
      "have made 0 api calls\n",
      "have made 0 api calls\n",
      "have made 20 api calls\n",
      "just finished HI. 42 states remaining\n",
      "ID\n",
      "max number of iterations is 393\n",
      "have made 0 api calls\n",
      "have made 0 api calls\n",
      "have made 0 api calls\n",
      "just finished ID. 41 states remaining\n",
      "IL\n",
      "max number of iterations is 506\n",
      "have made 0 api calls\n",
      "have made 20 api calls\n",
      "just finished IL. 40 states remaining\n",
      "IN\n",
      "max number of iterations is 379\n",
      "have made 0 api calls\n",
      "have made 0 api calls\n",
      "just finished IN. 39 states remaining\n",
      "IA\n",
      "max number of iterations is 395\n",
      "have made 0 api calls\n",
      "have made 0 api calls\n",
      "have made 0 api calls\n",
      "have made 0 api calls\n",
      "have made 0 api calls\n",
      "just finished IA. 38 states remaining\n",
      "KS\n",
      "max number of iterations is 626\n",
      "have made 0 api calls\n",
      "just finished KS. 37 states remaining\n",
      "KY\n",
      "max number of iterations is 447\n",
      "have made 0 api calls\n",
      "have made 0 api calls\n",
      "have made 0 api calls\n",
      "just finished KY. 36 states remaining\n",
      "LA\n",
      "max number of iterations is 449\n",
      "have made 0 api calls\n",
      "have made 0 api calls\n",
      "have made 0 api calls\n",
      "have made 0 api calls\n",
      "have made 0 api calls\n",
      "have made 20 api calls\n",
      "have made 20 api calls\n",
      "have made 20 api calls\n",
      "have made 20 api calls\n",
      "just finished LA. 35 states remaining\n",
      "ME\n",
      "max number of iterations is 219\n",
      "have made 0 api calls\n",
      "have made 0 api calls\n",
      "have made 20 api calls\n",
      "just finished ME. 34 states remaining\n",
      "MD\n",
      "max number of iterations is 225\n",
      "have made 0 api calls\n",
      "just finished MD. 33 states remaining\n",
      "MA\n",
      "max number of iterations is 274\n",
      "have made 0 api calls\n",
      "have made 0 api calls\n",
      "just finished MA. 32 states remaining\n",
      "MI\n",
      "max number of iterations is 589\n",
      "have made 0 api calls\n",
      "just finished MI. 31 states remaining\n",
      "MN\n",
      "max number of iterations is 426\n",
      "have made 0 api calls\n",
      "have made 0 api calls\n",
      "have made 0 api calls\n",
      "just finished MN. 30 states remaining\n",
      "MS\n",
      "max number of iterations is 315\n",
      "have made 0 api calls\n",
      "have made 0 api calls\n",
      "just finished MS. 29 states remaining\n",
      "MO\n",
      "max number of iterations is 521\n",
      "have made 0 api calls\n",
      "have made 0 api calls\n",
      "have made 20 api calls\n",
      "have made 20 api calls\n",
      "have made 20 api calls\n",
      "just finished MO. 28 states remaining\n",
      "MT\n",
      "max number of iterations is 698\n",
      "have made 0 api calls\n",
      "have made 0 api calls\n",
      "have made 20 api calls\n",
      "have made 20 api calls\n",
      "just finished MT. 27 states remaining\n",
      "NE\n",
      "max number of iterations is 527\n",
      "have made 0 api calls\n",
      "have made 0 api calls\n",
      "have made 0 api calls\n",
      "have made 0 api calls\n",
      "just finished NE. 26 states remaining\n",
      "NV\n",
      "max number of iterations is 332\n",
      "have made 0 api calls\n",
      "have made 0 api calls\n",
      "have made 20 api calls\n",
      "have made 20 api calls\n",
      "have made 20 api calls\n",
      "just finished NV. 25 states remaining\n",
      "NH\n",
      "max number of iterations is 210\n",
      "have made 0 api calls\n",
      "have made 0 api calls\n",
      "have made 0 api calls\n",
      "have made 0 api calls\n",
      "have made 0 api calls\n",
      "have made 0 api calls\n",
      "have made 0 api calls\n",
      "just finished NH. 24 states remaining\n",
      "NJ\n",
      "max number of iterations is 147\n",
      "have made 0 api calls\n",
      "just finished NJ. 23 states remaining\n",
      "NM\n",
      "max number of iterations is 661\n",
      "have made 0 api calls\n",
      "have made 0 api calls\n",
      "have made 0 api calls\n",
      "have made 0 api calls\n",
      "have made 0 api calls\n",
      "have made 0 api calls\n",
      "have made 0 api calls\n",
      "have made 0 api calls\n",
      "have made 0 api calls\n",
      "have made 0 api calls\n",
      "have made 0 api calls\n",
      "have made 0 api calls\n",
      "just finished NM. 22 states remaining\n",
      "NY\n",
      "max number of iterations is 775\n",
      "have made 0 api calls\n",
      "have made 0 api calls\n",
      "have made 0 api calls\n",
      "have made 0 api calls\n",
      "have made 0 api calls\n",
      "have made 0 api calls\n",
      "have made 0 api calls\n",
      "have made 0 api calls\n",
      "have made 0 api calls\n",
      "have made 0 api calls\n",
      "just finished NY. 21 states remaining\n",
      "NC\n",
      "max number of iterations is 475\n",
      "have made 0 api calls\n",
      "just finished NC. 20 states remaining\n",
      "ND\n",
      "max number of iterations is 294\n",
      "have made 0 api calls\n",
      "have made 20 api calls\n",
      "have made 20 api calls\n",
      "just finished ND. 19 states remaining\n",
      "OH\n",
      "max number of iterations is 485\n",
      "have made 0 api calls\n",
      "have made 0 api calls\n",
      "have made 0 api calls\n",
      "have made 0 api calls\n",
      "have made 0 api calls\n",
      "just finished OH. 18 states remaining\n",
      "OK\n",
      "max number of iterations is 489\n",
      "have made 0 api calls\n",
      "have made 0 api calls\n",
      "have made 0 api calls\n",
      "just finished OK. 17 states remaining\n",
      "OR\n",
      "max number of iterations is 638\n",
      "have made 0 api calls\n",
      "just finished OR. 16 states remaining\n",
      "PA\n",
      "max number of iterations is 699\n",
      "have made 0 api calls\n",
      "have made 0 api calls\n",
      "have made 0 api calls\n",
      "just finished PA. 15 states remaining\n",
      "RI\n",
      "max number of iterations is 33\n",
      "have made 0 api calls\n",
      "have made 0 api calls\n",
      "just finished RI. 14 states remaining\n",
      "SC\n",
      "max number of iterations is 215\n",
      "have made 0 api calls\n",
      "have made 0 api calls\n",
      "have made 0 api calls\n",
      "have made 0 api calls\n",
      "just finished SC. 13 states remaining\n",
      "SD\n",
      "max number of iterations is 376\n",
      "have made 0 api calls\n",
      "have made 0 api calls\n",
      "just finished SD. 12 states remaining\n",
      "TN\n",
      "max number of iterations is 503\n",
      "have made 0 api calls\n",
      "have made 0 api calls\n",
      "have made 0 api calls\n",
      "just finished TN. 11 states remaining\n",
      "TX\n",
      "max number of iterations is 1461\n",
      "have made 0 api calls\n",
      "have made 0 api calls\n",
      "have made 0 api calls\n",
      "have made 0 api calls\n",
      "have made 0 api calls\n",
      "have made 0 api calls\n",
      "have made 0 api calls\n",
      "have made 0 api calls\n",
      "just finished TX. 10 states remaining\n",
      "UT\n",
      "max number of iterations is 479\n",
      "have made 0 api calls\n",
      "have made 0 api calls\n",
      "have made 20 api calls\n",
      "have made 20 api calls\n",
      "have made 20 api calls\n",
      "have made 20 api calls\n",
      "have made 20 api calls\n",
      "just finished UT. 9 states remaining\n",
      "VT\n",
      "max number of iterations is 155\n",
      "have made 0 api calls\n",
      "just finished VT. 8 states remaining\n",
      "VA\n",
      "max number of iterations is 417\n",
      "have made 0 api calls\n",
      "have made 0 api calls\n",
      "have made 0 api calls\n",
      "have made 0 api calls\n",
      "just finished VA. 7 states remaining\n",
      "WA\n",
      "max number of iterations is 514\n",
      "have made 0 api calls\n",
      "just finished WA. 6 states remaining\n",
      "WV\n",
      "max number of iterations is 375\n",
      "have made 0 api calls\n",
      "have made 0 api calls\n",
      "have made 20 api calls\n",
      "just finished WV. 5 states remaining\n",
      "WI\n",
      "max number of iterations is 405\n",
      "have made 0 api calls\n",
      "just finished WI. 4 states remaining\n",
      "WY\n",
      "max number of iterations is 460\n",
      "have made 0 api calls\n",
      "have made 0 api calls\n",
      "have made 0 api calls\n",
      "have made 0 api calls\n",
      "have made 20 api calls\n",
      "just finished WY. 3 states remaining\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(100)\n",
    "\n",
    "# Dict of form {state:[list of station codes]}\n",
    "# state_station_codes = {}\n",
    "\n",
    "states_remaining = 50\n",
    "\n",
    "for state in state_indexes:\n",
    "    \"\"\"\n",
    "    Everything before the while loop is just to set up the while loop\n",
    "    \"\"\"\n",
    "    print(state)\n",
    "    \n",
    "    # List that will populate the values of state_station_codes dict\n",
    "    state_station_codes_values = []\n",
    "\n",
    "    # Keep track of how many stations we've added to the list. Only want to go to 10\n",
    "    count = 0\n",
    "    \n",
    "    # Keep track of how many times the while loop has run\n",
    "    tally = 0\n",
    "    \n",
    "    # Start and end indexes for each state\n",
    "    start_index = state_indexes[state][0]\n",
    "    end_index = state_indexes[state][1]\n",
    "    max_num_iters = end_index - start_index\n",
    "    print(f'max number of iterations is {max_num_iters}')\n",
    "    \n",
    "    # Keep track of used indexes to have no replacement\n",
    "    remaining_indexes = list(range(start_index,end_index+1))\n",
    "\n",
    "    while count < 10:\n",
    "\n",
    "        if not len(remaining_indexes):\n",
    "            break\n",
    "            \n",
    "        # Generate a random int to index from remaining_indexes\n",
    "        rand_index = np.random.randint(0, len(remaining_indexes))\n",
    "    \n",
    "        # Remove that index from remaining indexes so as to not resample\n",
    "        del remaining_indexes[rand_index]\n",
    "\n",
    "        # Call the API w/ that code for 1990-01-01\n",
    "        api_call = helper_functions.get_station_weather(split_lines[rand_index][0])\n",
    "        \n",
    "        # keep track of how many times it's called the api\n",
    "        if not tally % 20:\n",
    "            print(f'have made {tally} api calls')\n",
    "            \n",
    "        # restart loop if the api call yielded a blank entry\n",
    "        if not api_call:\n",
    "            continue\n",
    "        \n",
    "        # If output has both TMIN and TMAX then save that code and state\n",
    "        elif api_call[0].get('TMIN') and api_call[0].get('TMAX'):\n",
    "            state_station_codes_values.append(api_call[0]['STATION'])\n",
    "            count += 1\n",
    "        \n",
    "        tally += 1\n",
    "    \n",
    "    # After while loop is done, add state_station_codes_values to state_station_codes\n",
    "    state_station_codes[state] = state_station_codes_values\n",
    "    \n",
    "    print(f'just finished {state}. {states_remaining} states remaining')\n",
    "    states_remaining -= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AL 10\n",
      "AK 10\n",
      "AZ 10\n",
      "AR 10\n",
      "CA 10\n",
      "CO 10\n",
      "CT 10\n",
      "DE 2\n",
      "FL 10\n",
      "GA 10\n",
      "HI 10\n",
      "ID 10\n",
      "IL 10\n",
      "IN 10\n",
      "IA 10\n",
      "KS 10\n",
      "KY 10\n",
      "LA 10\n",
      "ME 10\n",
      "MD 10\n",
      "MA 10\n",
      "MI 10\n",
      "MN 10\n",
      "MS 10\n",
      "MO 10\n",
      "MT 10\n",
      "NE 10\n",
      "NV 10\n",
      "NH 10\n",
      "NJ 10\n",
      "NM 10\n",
      "NY 10\n",
      "NC 10\n",
      "ND 10\n",
      "OH 10\n",
      "OK 10\n",
      "OR 10\n",
      "PA 10\n",
      "RI 4\n",
      "SC 10\n",
      "SD 10\n",
      "TN 10\n",
      "TX 10\n",
      "UT 10\n",
      "VT 10\n",
      "VA 10\n",
      "WA 10\n",
      "WV 10\n",
      "WI 10\n",
      "WY 10\n"
     ]
    }
   ],
   "source": [
    "# Check how many stations in each state satisfied our requirements\n",
    "for state in state_station_codes:\n",
    "    print(state, len(state_station_codes[state]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gather and parse temperature data for each state from the API "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting AL. 50 states remaining\n",
      "Done gathering and parsing data for each station. Now making lists of temps by day\n",
      "Now finding average temps\n",
      "starting AK. 49 states remaining\n",
      "Done gathering and parsing data for each station. Now making lists of temps by day\n",
      "Now finding average temps\n",
      "starting AZ. 48 states remaining\n",
      "Done gathering and parsing data for each station. Now making lists of temps by day\n",
      "Now finding average temps\n",
      "starting AR. 47 states remaining\n",
      "Done gathering and parsing data for each station. Now making lists of temps by day\n",
      "Now finding average temps\n",
      "starting CA. 46 states remaining\n",
      "Done gathering and parsing data for each station. Now making lists of temps by day\n",
      "Now finding average temps\n",
      "starting CO. 45 states remaining\n",
      "Done gathering and parsing data for each station. Now making lists of temps by day\n",
      "Now finding average temps\n",
      "starting CT. 44 states remaining\n",
      "Done gathering and parsing data for each station. Now making lists of temps by day\n",
      "Now finding average temps\n",
      "starting DE. 43 states remaining\n",
      "Done gathering and parsing data for each station. Now making lists of temps by day\n",
      "Now finding average temps\n",
      "starting FL. 42 states remaining\n",
      "Done gathering and parsing data for each station. Now making lists of temps by day\n",
      "Now finding average temps\n",
      "starting GA. 41 states remaining\n",
      "Done gathering and parsing data for each station. Now making lists of temps by day\n",
      "Now finding average temps\n",
      "starting HI. 40 states remaining\n",
      "Done gathering and parsing data for each station. Now making lists of temps by day\n",
      "Now finding average temps\n",
      "starting ID. 39 states remaining\n",
      "Done gathering and parsing data for each station. Now making lists of temps by day\n",
      "Now finding average temps\n",
      "starting IL. 38 states remaining\n",
      "Done gathering and parsing data for each station. Now making lists of temps by day\n",
      "Now finding average temps\n",
      "starting IN. 37 states remaining\n",
      "Done gathering and parsing data for each station. Now making lists of temps by day\n",
      "Now finding average temps\n",
      "starting IA. 36 states remaining\n",
      "Done gathering and parsing data for each station. Now making lists of temps by day\n",
      "Now finding average temps\n",
      "starting KS. 35 states remaining\n",
      "Done gathering and parsing data for each station. Now making lists of temps by day\n",
      "Now finding average temps\n",
      "starting KY. 34 states remaining\n",
      "Done gathering and parsing data for each station. Now making lists of temps by day\n",
      "Now finding average temps\n",
      "starting LA. 33 states remaining\n",
      "Done gathering and parsing data for each station. Now making lists of temps by day\n",
      "Now finding average temps\n",
      "starting ME. 32 states remaining\n",
      "Done gathering and parsing data for each station. Now making lists of temps by day\n",
      "Now finding average temps\n",
      "starting MD. 31 states remaining\n",
      "Done gathering and parsing data for each station. Now making lists of temps by day\n",
      "Now finding average temps\n",
      "starting MA. 30 states remaining\n",
      "Done gathering and parsing data for each station. Now making lists of temps by day\n",
      "Now finding average temps\n",
      "starting MI. 29 states remaining\n",
      "Done gathering and parsing data for each station. Now making lists of temps by day\n",
      "Now finding average temps\n",
      "starting MN. 28 states remaining\n",
      "Done gathering and parsing data for each station. Now making lists of temps by day\n",
      "Now finding average temps\n",
      "starting MS. 27 states remaining\n",
      "Done gathering and parsing data for each station. Now making lists of temps by day\n",
      "Now finding average temps\n",
      "starting MO. 26 states remaining\n",
      "Done gathering and parsing data for each station. Now making lists of temps by day\n",
      "Now finding average temps\n",
      "starting MT. 25 states remaining\n",
      "Done gathering and parsing data for each station. Now making lists of temps by day\n",
      "Now finding average temps\n",
      "starting NE. 24 states remaining\n",
      "Done gathering and parsing data for each station. Now making lists of temps by day\n",
      "Now finding average temps\n",
      "starting NV. 23 states remaining\n",
      "Done gathering and parsing data for each station. Now making lists of temps by day\n",
      "Now finding average temps\n",
      "starting NH. 22 states remaining\n",
      "Done gathering and parsing data for each station. Now making lists of temps by day\n",
      "Now finding average temps\n",
      "starting NJ. 21 states remaining\n",
      "Done gathering and parsing data for each station. Now making lists of temps by day\n",
      "Now finding average temps\n",
      "starting NM. 20 states remaining\n",
      "Done gathering and parsing data for each station. Now making lists of temps by day\n",
      "Now finding average temps\n",
      "starting NY. 19 states remaining\n",
      "Done gathering and parsing data for each station. Now making lists of temps by day\n",
      "Now finding average temps\n",
      "starting NC. 18 states remaining\n",
      "Done gathering and parsing data for each station. Now making lists of temps by day\n",
      "Now finding average temps\n",
      "starting ND. 17 states remaining\n",
      "Done gathering and parsing data for each station. Now making lists of temps by day\n",
      "Now finding average temps\n",
      "starting OH. 16 states remaining\n",
      "Done gathering and parsing data for each station. Now making lists of temps by day\n",
      "Now finding average temps\n",
      "starting OK. 15 states remaining\n",
      "Done gathering and parsing data for each station. Now making lists of temps by day\n",
      "Now finding average temps\n",
      "starting OR. 14 states remaining\n",
      "Done gathering and parsing data for each station. Now making lists of temps by day\n",
      "Now finding average temps\n",
      "starting PA. 13 states remaining\n",
      "Done gathering and parsing data for each station. Now making lists of temps by day\n",
      "Now finding average temps\n",
      "starting RI. 12 states remaining\n",
      "Done gathering and parsing data for each station. Now making lists of temps by day\n",
      "Now finding average temps\n",
      "starting SC. 11 states remaining\n",
      "Done gathering and parsing data for each station. Now making lists of temps by day\n",
      "Now finding average temps\n",
      "starting SD. 10 states remaining\n",
      "Done gathering and parsing data for each station. Now making lists of temps by day\n",
      "Now finding average temps\n",
      "starting TN. 9 states remaining\n",
      "Done gathering and parsing data for each station. Now making lists of temps by day\n",
      "Now finding average temps\n",
      "starting TX. 8 states remaining\n",
      "Done gathering and parsing data for each station. Now making lists of temps by day\n",
      "Now finding average temps\n",
      "starting UT. 7 states remaining\n",
      "Done gathering and parsing data for each station. Now making lists of temps by day\n",
      "Now finding average temps\n",
      "starting VT. 6 states remaining\n",
      "Done gathering and parsing data for each station. Now making lists of temps by day\n",
      "Now finding average temps\n",
      "starting VA. 5 states remaining\n",
      "Done gathering and parsing data for each station. Now making lists of temps by day\n",
      "Now finding average temps\n",
      "starting WA. 4 states remaining\n",
      "Done gathering and parsing data for each station. Now making lists of temps by day\n",
      "Now finding average temps\n",
      "starting WV. 3 states remaining\n",
      "Done gathering and parsing data for each station. Now making lists of temps by day\n",
      "Now finding average temps\n",
      "starting WI. 2 states remaining\n",
      "Done gathering and parsing data for each station. Now making lists of temps by day\n",
      "Now finding average temps\n",
      "starting WY. 1 states remaining\n",
      "Done gathering and parsing data for each station. Now making lists of temps by day\n",
      "Now finding average temps\n"
     ]
    }
   ],
   "source": [
    "# Create empty container for final data to be uploaded to mongodb {'state' : [state data]}\n",
    "average_temp_data = {}\n",
    "\n",
    "states_left = 50\n",
    "\n",
    "for state in state_station_codes: #state_station_codes:\n",
    "    print(f'starting {state}. {states_left} states remaining')\n",
    "    \n",
    "    \n",
    "    # Create empty container for state-level data {'station': [station data]}\n",
    "    state_data = {}\n",
    "    \n",
    "    # Iterate through each station to get all of its weather data \n",
    "    for station in state_station_codes[state]:\n",
    "        \n",
    "        # Create empty container for station-level data [{'DATE':date, 'TMIN':tmin, 'TMAX':tmax}]\n",
    "        station_data = []\n",
    "        \n",
    "        # Call api for this station to collect weather data\n",
    "        api_call = helper_functions.get_station_weather(station,'1960-01-01','2018-12-31')\n",
    "        \n",
    "        \n",
    "        for day in api_call:\n",
    "            \n",
    "            # Create empty container for daily-level data\n",
    "            daily_data = {}\n",
    "            \n",
    "            # For each dict in the response, save 'DATE', api_call.get('STATION'), api_call.get('TMIN'), api_call.get('TMAX'), api_call.get('STATE')\n",
    "            daily_data['DATE'] = day.get('DATE')\n",
    "            daily_data['TMIN'] = day.get('TMIN')\n",
    "            daily_data['TMAX'] = day.get('TMAX') \n",
    "\n",
    "            # Append to station level-data\n",
    "            station_data.append(daily_data)\n",
    "        \n",
    "        # Append to state-level data\n",
    "        state_data[station] = station_data        \n",
    "    \n",
    "    print('Done gathering and parsing data for each station. Now making lists of temps by day')\n",
    "    \n",
    "    # Loop through each station and add as ints to the appropriate numpy array\n",
    "    dates_temp = {}\n",
    "\n",
    "    # Iterate through each station within a state\n",
    "    for station in state_data:\n",
    "#         print(station)\n",
    "\n",
    "        # Iterate through each day of data from a particular station to get TMIN and TMAX\n",
    "        for daily_data in state_data[station]:\n",
    "    #         print(daily_data)\n",
    "\n",
    "            # Only do stuff if 1. it's not an empty list and 2. the temperature data is present\n",
    "            if daily_data and (daily_data.get('TMAX') and daily_data.get('TMIN')):\n",
    "\n",
    "                # Keep track of the date\n",
    "                date = daily_data['DATE']\n",
    "\n",
    "                # Create a key within our dict that keeps track of the date\n",
    "                if not dates_temp.get(date):\n",
    "\n",
    "                    # Assign empty lists where we'll store info from each station\n",
    "                    dates_temp[date] = {'TMAX':[], 'TMIN':[]}\n",
    "\n",
    "                # Append    \n",
    "                dates_temp[date]['TMAX'].append(int(daily_data['TMAX']))\n",
    "                dates_temp[date]['TMIN'].append(int(daily_data['TMIN']))\n",
    "    \n",
    "    print('Now finding average temps')\n",
    "    \n",
    "    # Loop through the arrays and store the mean value into final dict\n",
    "    final_values = []\n",
    "    \n",
    "    # Iterate through dates_temp TMIN/TMAX arrays and store the mean value into final dict\n",
    "    for date in dates_temp:\n",
    "\n",
    "        daily_temps = {}\n",
    "        daily_temps['date'] = date\n",
    "        daily_temps['tmax'] = round(np.mean(np.array(dates_temp[date]['TMAX'])),2)\n",
    "        daily_temps['tmin'] = round(np.mean(np.array(dates_temp[date]['TMIN'])),2)\n",
    "        final_values.append(daily_temps)\n",
    "    \n",
    "    average_temp_data[state] = final_values\n",
    "    states_left -= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AL 21550\n",
      "AK 21550\n",
      "AZ 21550\n",
      "AR 21550\n",
      "CA 21550\n",
      "CO 21550\n",
      "CT 21550\n",
      "DE 21139\n",
      "FL 21550\n",
      "GA 21550\n",
      "HI 21550\n",
      "ID 21550\n",
      "IL 21550\n",
      "IN 21550\n",
      "IA 21550\n",
      "KS 21550\n",
      "KY 21550\n",
      "LA 21550\n",
      "ME 21550\n",
      "MD 21550\n",
      "MA 21550\n",
      "MI 21550\n",
      "MN 21550\n",
      "MS 21550\n",
      "MO 21550\n",
      "MT 21550\n",
      "NE 21550\n",
      "NV 21550\n",
      "NH 21550\n",
      "NJ 21550\n",
      "NM 21550\n",
      "NY 21550\n",
      "NC 21550\n",
      "ND 21550\n",
      "OH 21549\n",
      "OK 21550\n",
      "OR 21550\n",
      "PA 21550\n",
      "RI 21550\n",
      "SC 21550\n",
      "SD 21550\n",
      "TN 21550\n",
      "TX 21550\n",
      "UT 21550\n",
      "VT 21550\n",
      "VA 21550\n",
      "WA 21550\n",
      "WV 21550\n",
      "WI 21550\n",
      "WY 21550\n"
     ]
    }
   ],
   "source": [
    "# Check how many data points per state\n",
    "for state in average_temp_data:\n",
    "    print(state, len(average_temp_data[state]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, I forgot all of the keys are now in state abbreviations, so I'm going to switch them all to the normal state name for easier querying from the db later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_abbrevs = open('state-abbreviations.csv')\n",
    "state_abbrevs_reader = csv.reader(state_abbrevs)\n",
    "state_abbrevs_dict = dict(state_abbrevs_reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_temp_data = {}\n",
    "\n",
    "for state in average_temp_data:\n",
    "    full_state_name = state_abbrevs_dict[state]\n",
    "    final_temp_data[full_state_name] = average_temp_data[state]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out this file is too big to fit onto mongo. I'll therefore separate it into a list of dicts or make this cool trick\n",
    "\n",
    "count number of days in each year that:\n",
    "\n",
    "Temp is above 70, 75, 80, 85, 90, 95, or 100\n",
    "\n",
    "Below 70, 65, 60, 55, 50, 45, 40, 35, 30, 25, 20, 15, 10, 5, or 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Alabama', 'Alaska', 'Arizona', 'Arkansas', 'California', 'Colorado', 'Connecticut', 'Delaware', 'Florida', 'Georgia', 'Hawaii', 'Idaho', 'Illinois', 'Indiana', 'Iowa', 'Kansas', 'Kentucky', 'Louisiana', 'Maine', 'Maryland', 'Massachusetts', 'Michigan', 'Minnesota', 'Mississippi', 'Missouri', 'Montana', 'Nebraska', 'Nevada', 'New Hampshire', 'New Jersey', 'New Mexico', 'New York', 'North Carolina', 'North Dakota', 'Ohio', 'Oklahoma', 'Oregon', 'Pennsylvania', 'Rhode Island', 'South Carolina', 'South Dakota', 'Tennessee', 'Texas', 'Utah', 'Vermont', 'Virginia', 'Washington', 'West Virginia', 'Wisconsin', 'Wyoming'])"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_temp_data.pop('_id')\n",
    "final_temp_data.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Engineering temperature data to have information on a yearly basis rather than daily basis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The below code goes through the final temp data to make new lists of just the TMAX values \n",
    "for each year so that we can start getting year-level numbers that could be useful.\n",
    "\"\"\"\n",
    "\n",
    "# Create container for final output\n",
    "processed_temp_data = []\n",
    "\n",
    "# Iterate through each state's data\n",
    "for state in final_temp_data:\n",
    "    \n",
    "    # Create container to store each state's individual yearly data\n",
    "    years_temps_lists = {str(year):[] for year in np.arange(1960,2019)}\n",
    "\n",
    "    # Iterate through all of a state's daily data\n",
    "    for day in final_temp_data[state]:\n",
    "\n",
    "        # Figure out which key to access in years_temps_lists and assign the tmax\n",
    "        year = day['date'][:4]\n",
    "        years_temps_lists[year].append(day['tmax'])\n",
    "\n",
    "    # Create arrays with which to determine how many days in a year were above certain temps    \n",
    "    above_temps = np.arange(100, 65, -5)\n",
    "    below_temps = np.arange(70, 0, -5)\n",
    "\n",
    "    # Create container for each state's output\n",
    "    yearly_above_below = {}\n",
    "    \n",
    "    # Iterate through the data which has now been sorted into yearly chunks\n",
    "    for year in years_temps_lists:\n",
    "        \n",
    "        # Convert to array for quick math\n",
    "        array = np.array(years_temps_lists[year])\n",
    "\n",
    "        # Find number of days that the temperature was above/below a certain threshold in a given year\n",
    "        # Have to convert to int because mongo doesn't accept numpy.int64\n",
    "        days_above_temp = [int((array>temp).sum()) for temp in above_temps]\n",
    "        days_below_temp = [int((array<temp).sum()) for temp in below_temps]\n",
    "\n",
    "        # Assign this information to a year\n",
    "        yearly_above_below[year] = [days_above_temp, days_below_temp]\n",
    "    \n",
    "    # Append each state's data to the container\n",
    "    processed_temp_data.append({'state' : state, 'data' : yearly_above_below, 'description':'Temperature','Units':'F'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "int"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(processed_temp_data[0]['data']['1960'][0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking to make sure we have a complete data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print any states that don't have a complete data set\n",
    "for state in states_data:\n",
    "    data = state['data']\n",
    "    if len(data) < len(np.arange(1960,2019)):\n",
    "        print(state['state'])\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storing data into MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'asserts': {'msg': 0, 'regular': 0, 'rollovers': 0, 'user': 25, 'warning': 0},\n",
      " 'connections': {'active': 1,\n",
      "                 'available': 3271,\n",
      "                 'current': 5,\n",
      "                 'totalCreated': 18},\n",
      " 'electionMetrics': {'averageCatchUpOps': 0.0,\n",
      "                     'catchUpTakeover': {'called': 0, 'successful': 0},\n",
      "                     'electionTimeout': {'called': 0, 'successful': 0},\n",
      "                     'freezeTimeout': {'called': 0, 'successful': 0},\n",
      "                     'numCatchUps': 0,\n",
      "                     'numCatchUpsAlreadyCaughtUp': 0,\n",
      "                     'numCatchUpsFailedWithError': 0,\n",
      "                     'numCatchUpsFailedWithNewTerm': 0,\n",
      "                     'numCatchUpsFailedWithReplSetAbortPrimaryCatchUpCmd': 0,\n",
      "                     'numCatchUpsSkipped': 0,\n",
      "                     'numCatchUpsSucceeded': 0,\n",
      "                     'numCatchUpsTimedOut': 0,\n",
      "                     'numStepDownsCausedByHigherTerm': 0,\n",
      "                     'priorityTakeover': {'called': 0, 'successful': 0},\n",
      "                     'stepUpCmd': {'called': 0, 'successful': 0}},\n",
      " 'extra_info': {'note': 'fields vary by platform', 'page_faults': 3},\n",
      " 'flowControl': {'enabled': True,\n",
      "                 'isLagged': False,\n",
      "                 'isLaggedCount': 0,\n",
      "                 'isLaggedTimeMicros': 0,\n",
      "                 'locksPerOp': 0.0,\n",
      "                 'sustainerRate': 0,\n",
      "                 'targetRateLimit': 1000000000,\n",
      "                 'timeAcquiringMicros': 1850},\n",
      " 'freeMonitoring': {'lastRunTime': '2020-01-13T11:35:56.965-0500',\n",
      "                    'metricsErrors': 0,\n",
      "                    'registerErrors': 0,\n",
      "                    'retryIntervalSecs': 60,\n",
      "                    'state': 'enabled'},\n",
      " 'globalLock': {'activeClients': {'readers': 0, 'total': 0, 'writers': 0},\n",
      "                'currentQueue': {'readers': 0, 'total': 0, 'writers': 0},\n",
      "                'totalTime': 331197976000},\n",
      " 'host': 'Marcos-MacBook-Pro.local',\n",
      " 'localTime': datetime.datetime(2020, 1, 13, 16, 36, 0, 806000),\n",
      " 'locks': {'Collection': {'acquireCount': {'R': 1,\n",
      "                                           'W': 3,\n",
      "                                           'r': 124202,\n",
      "                                           'w': 2075}},\n",
      "           'Database': {'acquireCount': {'W': 13, 'r': 123803, 'w': 2070}},\n",
      "           'Global': {'acquireCount': {'W': 4, 'r': 351430, 'w': 2082}},\n",
      "           'Mutex': {'acquireCount': {'r': 134480}},\n",
      "           'ParallelBatchWriterMode': {'acquireCount': {'r': 4044}},\n",
      "           'ReplicationStateTransition': {'acquireCount': {'w': 353516}},\n",
      "           'oplog': {'acquireCount': {'r': 116550}}},\n",
      " 'logicalSessionRecordCache': {'activeSessionsCount': 2,\n",
      "                               'lastSessionsCollectionJobCursorsClosed': 0,\n",
      "                               'lastSessionsCollectionJobDurationMillis': 0,\n",
      "                               'lastSessionsCollectionJobEntriesEnded': 0,\n",
      "                               'lastSessionsCollectionJobEntriesRefreshed': 1,\n",
      "                               'lastSessionsCollectionJobTimestamp': datetime.datetime(2020, 1, 13, 16, 32, 34, 56000),\n",
      "                               'lastTransactionReaperJobDurationMillis': 0,\n",
      "                               'lastTransactionReaperJobEntriesCleanedUp': 0,\n",
      "                               'lastTransactionReaperJobTimestamp': datetime.datetime(2020, 1, 13, 16, 32, 34, 56000),\n",
      "                               'sessionCatalogSize': 0,\n",
      "                               'sessionsCollectionJobCount': 389,\n",
      "                               'transactionReaperJobCount': 389},\n",
      " 'mem': {'bits': 64, 'resident': 36, 'supported': True, 'virtual': 5401},\n",
      " 'metrics': {'commands': {'<UNKNOWN>': 0,\n",
      "                          '_addShard': {'failed': 0, 'total': 0},\n",
      "                          '_cloneCatalogData': {'failed': 0, 'total': 0},\n",
      "                          '_cloneCollectionOptionsFromPrimaryShard': {'failed': 0,\n",
      "                                                                      'total': 0},\n",
      "                          '_configsvrAddShard': {'failed': 0, 'total': 0},\n",
      "                          '_configsvrAddShardToZone': {'failed': 0, 'total': 0},\n",
      "                          '_configsvrBalancerStart': {'failed': 0, 'total': 0},\n",
      "                          '_configsvrBalancerStatus': {'failed': 0, 'total': 0},\n",
      "                          '_configsvrBalancerStop': {'failed': 0, 'total': 0},\n",
      "                          '_configsvrCommitChunkMerge': {'failed': 0,\n",
      "                                                         'total': 0},\n",
      "                          '_configsvrCommitChunkMigration': {'failed': 0,\n",
      "                                                             'total': 0},\n",
      "                          '_configsvrCommitChunkSplit': {'failed': 0,\n",
      "                                                         'total': 0},\n",
      "                          '_configsvrCommitMovePrimary': {'failed': 0,\n",
      "                                                          'total': 0},\n",
      "                          '_configsvrCreateCollection': {'failed': 0,\n",
      "                                                         'total': 0},\n",
      "                          '_configsvrCreateDatabase': {'failed': 0, 'total': 0},\n",
      "                          '_configsvrDropCollection': {'failed': 0, 'total': 0},\n",
      "                          '_configsvrDropDatabase': {'failed': 0, 'total': 0},\n",
      "                          '_configsvrEnableSharding': {'failed': 0, 'total': 0},\n",
      "                          '_configsvrMoveChunk': {'failed': 0, 'total': 0},\n",
      "                          '_configsvrMovePrimary': {'failed': 0, 'total': 0},\n",
      "                          '_configsvrRemoveShard': {'failed': 0, 'total': 0},\n",
      "                          '_configsvrRemoveShardFromZone': {'failed': 0,\n",
      "                                                            'total': 0},\n",
      "                          '_configsvrShardCollection': {'failed': 0,\n",
      "                                                        'total': 0},\n",
      "                          '_configsvrUpdateZoneKeyRange': {'failed': 0,\n",
      "                                                           'total': 0},\n",
      "                          '_flushDatabaseCacheUpdates': {'failed': 0,\n",
      "                                                         'total': 0},\n",
      "                          '_flushRoutingTableCacheUpdates': {'failed': 0,\n",
      "                                                             'total': 0},\n",
      "                          '_getNextSessionMods': {'failed': 0, 'total': 0},\n",
      "                          '_getUserCacheGeneration': {'failed': 0, 'total': 0},\n",
      "                          '_isSelf': {'failed': 0, 'total': 0},\n",
      "                          '_mergeAuthzCollections': {'failed': 0, 'total': 0},\n",
      "                          '_migrateClone': {'failed': 0, 'total': 0},\n",
      "                          '_movePrimary': {'failed': 0, 'total': 0},\n",
      "                          '_recvChunkAbort': {'failed': 0, 'total': 0},\n",
      "                          '_recvChunkCommit': {'failed': 0, 'total': 0},\n",
      "                          '_recvChunkStart': {'failed': 0, 'total': 0},\n",
      "                          '_recvChunkStatus': {'failed': 0, 'total': 0},\n",
      "                          '_shardsvrShardCollection': {'failed': 0, 'total': 0},\n",
      "                          '_transferMods': {'failed': 0, 'total': 0},\n",
      "                          'abortTransaction': {'failed': 0, 'total': 0},\n",
      "                          'aggregate': {'failed': 0, 'total': 0},\n",
      "                          'appendOplogNote': {'failed': 0, 'total': 0},\n",
      "                          'applyOps': {'failed': 0, 'total': 0},\n",
      "                          'authenticate': {'failed': 0, 'total': 0},\n",
      "                          'availableQueryOptions': {'failed': 0, 'total': 0},\n",
      "                          'buildInfo': {'failed': 0, 'total': 2},\n",
      "                          'checkShardingIndex': {'failed': 0, 'total': 0},\n",
      "                          'cleanupOrphaned': {'failed': 0, 'total': 0},\n",
      "                          'cloneCollection': {'failed': 0, 'total': 0},\n",
      "                          'cloneCollectionAsCapped': {'failed': 0, 'total': 0},\n",
      "                          'collMod': {'failed': 0, 'total': 0},\n",
      "                          'collStats': {'failed': 0, 'total': 0},\n",
      "                          'commitTransaction': {'failed': 0, 'total': 0},\n",
      "                          'compact': {'failed': 0, 'total': 0},\n",
      "                          'connPoolStats': {'failed': 0, 'total': 0},\n",
      "                          'connPoolSync': {'failed': 0, 'total': 0},\n",
      "                          'connectionStatus': {'failed': 0, 'total': 0},\n",
      "                          'convertToCapped': {'failed': 0, 'total': 0},\n",
      "                          'coordinateCommitTransaction': {'failed': 0,\n",
      "                                                          'total': 0},\n",
      "                          'count': {'failed': 0, 'total': 0},\n",
      "                          'create': {'failed': 0, 'total': 0},\n",
      "                          'createIndexes': {'failed': 0, 'total': 1},\n",
      "                          'createRole': {'failed': 0, 'total': 0},\n",
      "                          'createUser': {'failed': 0, 'total': 0},\n",
      "                          'currentOp': {'failed': 0, 'total': 0},\n",
      "                          'dataSize': {'failed': 0, 'total': 0},\n",
      "                          'dbHash': {'failed': 0, 'total': 0},\n",
      "                          'dbStats': {'failed': 0, 'total': 0},\n",
      "                          'delete': {'failed': 0, 'total': 2},\n",
      "                          'distinct': {'failed': 0, 'total': 0},\n",
      "                          'driverOIDTest': {'failed': 0, 'total': 0},\n",
      "                          'drop': {'failed': 0, 'total': 0},\n",
      "                          'dropAllRolesFromDatabase': {'failed': 0, 'total': 0},\n",
      "                          'dropAllUsersFromDatabase': {'failed': 0, 'total': 0},\n",
      "                          'dropConnections': {'failed': 0, 'total': 0},\n",
      "                          'dropDatabase': {'failed': 0, 'total': 0},\n",
      "                          'dropIndexes': {'failed': 0, 'total': 0},\n",
      "                          'dropRole': {'failed': 0, 'total': 0},\n",
      "                          'dropUser': {'failed': 0, 'total': 0},\n",
      "                          'endSessions': {'failed': 0, 'total': 0},\n",
      "                          'explain': {'failed': 0, 'total': 0},\n",
      "                          'features': {'failed': 0, 'total': 0},\n",
      "                          'filemd5': {'failed': 0, 'total': 0},\n",
      "                          'find': {'failed': 0, 'total': 402},\n",
      "                          'findAndModify': {'failed': 0, 'total': 0},\n",
      "                          'flushRouterConfig': {'failed': 0, 'total': 0},\n",
      "                          'fsync': {'failed': 0, 'total': 0},\n",
      "                          'fsyncUnlock': {'failed': 0, 'total': 0},\n",
      "                          'geoSearch': {'failed': 0, 'total': 0},\n",
      "                          'getCmdLineOpts': {'failed': 0, 'total': 0},\n",
      "                          'getDatabaseVersion': {'failed': 0, 'total': 0},\n",
      "                          'getDiagnosticData': {'failed': 0, 'total': 0},\n",
      "                          'getFreeMonitoringStatus': {'failed': 0, 'total': 2},\n",
      "                          'getLastError': {'failed': 0, 'total': 0},\n",
      "                          'getLog': {'failed': 0, 'total': 1},\n",
      "                          'getMore': {'failed': 0, 'total': 3},\n",
      "                          'getParameter': {'failed': 0, 'total': 0},\n",
      "                          'getShardMap': {'failed': 0, 'total': 0},\n",
      "                          'getShardVersion': {'failed': 0, 'total': 0},\n",
      "                          'getnonce': {'failed': 0, 'total': 0},\n",
      "                          'grantPrivilegesToRole': {'failed': 0, 'total': 0},\n",
      "                          'grantRolesToRole': {'failed': 0, 'total': 0},\n",
      "                          'grantRolesToUser': {'failed': 0, 'total': 0},\n",
      "                          'hostInfo': {'failed': 0, 'total': 0},\n",
      "                          'insert': {'failed': 0, 'total': 5},\n",
      "                          'invalidateUserCache': {'failed': 0, 'total': 0},\n",
      "                          'isMaster': {'failed': 0, 'total': 5651},\n",
      "                          'killAllSessions': {'failed': 0, 'total': 0},\n",
      "                          'killAllSessionsByPattern': {'failed': 0, 'total': 0},\n",
      "                          'killCursors': {'failed': 0, 'total': 0},\n",
      "                          'killOp': {'failed': 0, 'total': 0},\n",
      "                          'killSessions': {'failed': 0, 'total': 0},\n",
      "                          'listCollections': {'failed': 0, 'total': 0},\n",
      "                          'listCommands': {'failed': 0, 'total': 0},\n",
      "                          'listDatabases': {'failed': 0, 'total': 5},\n",
      "                          'listIndexes': {'failed': 2, 'total': 778},\n",
      "                          'lockInfo': {'failed': 0, 'total': 0},\n",
      "                          'logRotate': {'failed': 0, 'total': 0},\n",
      "                          'logout': {'failed': 0, 'total': 0},\n",
      "                          'mapReduce': {'failed': 0, 'total': 0},\n",
      "                          'mapreduce': {'shardedfinish': {'failed': 0,\n",
      "                                                          'total': 0}},\n",
      "                          'mergeChunks': {'failed': 0, 'total': 0},\n",
      "                          'moveChunk': {'failed': 0, 'total': 0},\n",
      "                          'ping': {'failed': 0, 'total': 0},\n",
      "                          'planCacheClear': {'failed': 0, 'total': 0},\n",
      "                          'planCacheClearFilters': {'failed': 0, 'total': 0},\n",
      "                          'planCacheListFilters': {'failed': 0, 'total': 0},\n",
      "                          'planCacheListPlans': {'failed': 0, 'total': 0},\n",
      "                          'planCacheListQueryShapes': {'failed': 0, 'total': 0},\n",
      "                          'planCacheSetFilter': {'failed': 0, 'total': 0},\n",
      "                          'prepareTransaction': {'failed': 0, 'total': 0},\n",
      "                          'profile': {'failed': 0, 'total': 0},\n",
      "                          'reIndex': {'failed': 0, 'total': 0},\n",
      "                          'refreshSessions': {'failed': 0, 'total': 0},\n",
      "                          'renameCollection': {'failed': 0, 'total': 0},\n",
      "                          'repairCursor': {'failed': 0, 'total': 0},\n",
      "                          'repairDatabase': {'failed': 0, 'total': 0},\n",
      "                          'replSetAbortPrimaryCatchUp': {'failed': 0,\n",
      "                                                         'total': 0},\n",
      "                          'replSetFreeze': {'failed': 0, 'total': 0},\n",
      "                          'replSetGetConfig': {'failed': 0, 'total': 0},\n",
      "                          'replSetGetRBID': {'failed': 0, 'total': 0},\n",
      "                          'replSetGetStatus': {'failed': 1, 'total': 1},\n",
      "                          'replSetHeartbeat': {'failed': 0, 'total': 0},\n",
      "                          'replSetInitiate': {'failed': 0, 'total': 0},\n",
      "                          'replSetMaintenance': {'failed': 0, 'total': 0},\n",
      "                          'replSetReconfig': {'failed': 0, 'total': 0},\n",
      "                          'replSetRequestVotes': {'failed': 0, 'total': 0},\n",
      "                          'replSetResizeOplog': {'failed': 0, 'total': 0},\n",
      "                          'replSetStepDown': {'failed': 0, 'total': 0},\n",
      "                          'replSetStepDownWithForce': {'failed': 0, 'total': 0},\n",
      "                          'replSetStepUp': {'failed': 0, 'total': 0},\n",
      "                          'replSetSyncFrom': {'failed': 0, 'total': 0},\n",
      "                          'replSetUpdatePosition': {'failed': 0, 'total': 0},\n",
      "                          'resetError': {'failed': 0, 'total': 0},\n",
      "                          'revokePrivilegesFromRole': {'failed': 0, 'total': 0},\n",
      "                          'revokeRolesFromRole': {'failed': 0, 'total': 0},\n",
      "                          'revokeRolesFromUser': {'failed': 0, 'total': 0},\n",
      "                          'rolesInfo': {'failed': 0, 'total': 0},\n",
      "                          'saslContinue': {'failed': 0, 'total': 0},\n",
      "                          'saslStart': {'failed': 0, 'total': 0},\n",
      "                          'serverStatus': {'failed': 0, 'total': 8},\n",
      "                          'setFeatureCompatibilityVersion': {'failed': 0,\n",
      "                                                             'total': 0},\n",
      "                          'setFreeMonitoring': {'failed': 0, 'total': 1},\n",
      "                          'setIndexCommitQuorum': {'failed': 0, 'total': 0},\n",
      "                          'setParameter': {'failed': 0, 'total': 0},\n",
      "                          'setShardVersion': {'failed': 0, 'total': 0},\n",
      "                          'shardConnPoolStats': {'failed': 0, 'total': 0},\n",
      "                          'shardingState': {'failed': 0, 'total': 0},\n",
      "                          'shutdown': {'failed': 0, 'total': 0},\n",
      "                          'splitChunk': {'failed': 0, 'total': 0},\n",
      "                          'splitVector': {'failed': 0, 'total': 0},\n",
      "                          'startRecordingTraffic': {'failed': 0, 'total': 0},\n",
      "                          'startSession': {'failed': 0, 'total': 0},\n",
      "                          'stopRecordingTraffic': {'failed': 0, 'total': 0},\n",
      "                          'top': {'failed': 0, 'total': 0},\n",
      "                          'touch': {'failed': 0, 'total': 0},\n",
      "                          'unsetSharding': {'failed': 0, 'total': 0},\n",
      "                          'update': {'failed': 0, 'total': 15},\n",
      "                          'updateRole': {'failed': 0, 'total': 0},\n",
      "                          'updateUser': {'failed': 0, 'total': 0},\n",
      "                          'usersInfo': {'failed': 0, 'total': 0},\n",
      "                          'validate': {'failed': 0, 'total': 0},\n",
      "                          'voteCommitIndexBuild': {'failed': 0, 'total': 0},\n",
      "                          'waitForFailPoint': {'failed': 0, 'total': 0},\n",
      "                          'whatsmyuri': {'failed': 0, 'total': 1}},\n",
      "             'cursor': {'open': {'noTimeout': 0, 'pinned': 0, 'total': 0},\n",
      "                        'timedOut': 0},\n",
      "             'document': {'deleted': 2184,\n",
      "                          'inserted': 4418,\n",
      "                          'returned': 6893,\n",
      "                          'updated': 8},\n",
      "             'getLastError': {'wtime': {'num': 0, 'totalMillis': 0},\n",
      "                              'wtimeouts': 0},\n",
      "             'operation': {'scanAndOrder': 0, 'writeConflicts': 0},\n",
      "             'query': {'planCacheTotalSizeEstimateBytes': 0,\n",
      "                       'updateOneOpStyleBroadcastWithExactIDCount': 0},\n",
      "             'queryExecutor': {'scanned': 8, 'scannedObjects': 28551},\n",
      "             'record': {'moves': 0},\n",
      "             'repl': {'apply': {'attemptsToBecomeSecondary': 0,\n",
      "                                'batchSize': 0,\n",
      "                                'batches': {'num': 0, 'totalMillis': 0},\n",
      "                                'ops': 0},\n",
      "                      'buffer': {'count': 0, 'maxSizeBytes': 0, 'sizeBytes': 0},\n",
      "                      'executor': {'networkInterface': 'DEPRECATED: '\n",
      "                                                       'getDiagnosticString is '\n",
      "                                                       'deprecated in '\n",
      "                                                       'NetworkInterfaceTL',\n",
      "                                   'pool': {'inProgressCount': 0},\n",
      "                                   'queues': {'networkInProgress': 0,\n",
      "                                              'sleepers': 0},\n",
      "                                   'shuttingDown': False,\n",
      "                                   'unsignaledEvents': 0},\n",
      "                      'initialSync': {'completed': 0,\n",
      "                                      'failedAttempts': 0,\n",
      "                                      'failures': 0},\n",
      "                      'network': {'bytes': 0,\n",
      "                                  'getmores': {'num': 0, 'totalMillis': 0},\n",
      "                                  'notMasterLegacyUnacknowledgedWrites': 0,\n",
      "                                  'notMasterUnacknowledgedWrites': 0,\n",
      "                                  'ops': 0,\n",
      "                                  'readersCreated': 0},\n",
      "                      'stepDown': {'userOperationsKilled': 0,\n",
      "                                   'userOperationsRunning': 0}},\n",
      "             'ttl': {'deletedDocuments': 6, 'passes': 1942}},\n",
      " 'network': {'bytesIn': 8855219,\n",
      "             'bytesOut': 14397163,\n",
      "             'compression': {'snappy': {'compressor': {'bytesIn': 0,\n",
      "                                                       'bytesOut': 0},\n",
      "                                        'decompressor': {'bytesIn': 0,\n",
      "                                                         'bytesOut': 0}},\n",
      "                             'zlib': {'compressor': {'bytesIn': 0,\n",
      "                                                     'bytesOut': 0},\n",
      "                                      'decompressor': {'bytesIn': 0,\n",
      "                                                       'bytesOut': 0}},\n",
      "                             'zstd': {'compressor': {'bytesIn': 0,\n",
      "                                                     'bytesOut': 0},\n",
      "                                      'decompressor': {'bytesIn': 0,\n",
      "                                                       'bytesOut': 0}}},\n",
      "             'numRequests': 5695,\n",
      "             'physicalBytesIn': 8855219,\n",
      "             'physicalBytesOut': 14397163,\n",
      "             'serviceExecutorTaskStats': {'executor': 'passthrough',\n",
      "                                          'threadsRunning': 5}},\n",
      " 'ok': 1.0,\n",
      " 'opLatencies': {'commands': {'latency': 381425, 'ops': 5671},\n",
      "                 'reads': {'latency': 40141, 'ops': 16},\n",
      "                 'transactions': {'latency': 0, 'ops': 0},\n",
      "                 'writes': {'latency': 118647, 'ops': 7}},\n",
      " 'opReadConcernCounters': {'available': 0,\n",
      "                           'linearizable': 0,\n",
      "                           'local': 0,\n",
      "                           'majority': 0,\n",
      "                           'none': 402,\n",
      "                           'snapshot': 0},\n",
      " 'opcounters': {'command': 6451,\n",
      "                'delete': 2,\n",
      "                'getmore': 3,\n",
      "                'insert': 4418,\n",
      "                'query': 402,\n",
      "                'update': 16},\n",
      " 'opcountersRepl': {'command': 0,\n",
      "                    'delete': 0,\n",
      "                    'getmore': 0,\n",
      "                    'insert': 0,\n",
      "                    'query': 0,\n",
      "                    'update': 0},\n",
      " 'pid': 48245,\n",
      " 'process': 'mongod',\n",
      " 'storageEngine': {'backupCursorOpen': False,\n",
      "                   'dropPendingIdents': 0,\n",
      "                   'name': 'wiredTiger',\n",
      "                   'oldestRequiredTimestampForCrashRecovery': Timestamp(0, 0),\n",
      "                   'persistent': True,\n",
      "                   'readOnly': False,\n",
      "                   'supportsCommittedReads': True,\n",
      "                   'supportsPendingDrops': True,\n",
      "                   'supportsSnapshotReadConcern': True},\n",
      " 'trafficRecording': {'running': False},\n",
      " 'transactions': {'currentActive': 0,\n",
      "                  'currentInactive': 0,\n",
      "                  'currentOpen': 0,\n",
      "                  'currentPrepared': 0,\n",
      "                  'retriedCommandsCount': 0,\n",
      "                  'retriedStatementsCount': 0,\n",
      "                  'totalAborted': 0,\n",
      "                  'totalCommitted': 0,\n",
      "                  'totalPrepared': 0,\n",
      "                  'totalPreparedThenAborted': 0,\n",
      "                  'totalPreparedThenCommitted': 0,\n",
      "                  'totalStarted': 0,\n",
      "                  'transactionsCollectionWriteCount': 0},\n",
      " 'transportSecurity': {'1.0': 0, '1.1': 0, '1.2': 0, '1.3': 0, 'unknown': 0},\n",
      " 'twoPhaseCommitCoordinator': {'currentInSteps': {'deletingCoordinatorDoc': 0,\n",
      "                                                  'waitingForDecisionAcks': 0,\n",
      "                                                  'waitingForVotes': 0,\n",
      "                                                  'writingDecision': 0,\n",
      "                                                  'writingParticipantList': 0},\n",
      "                               'totalAbortedTwoPhaseCommit': 0,\n",
      "                               'totalCommittedTwoPhaseCommit': 0,\n",
      "                               'totalCreated': 0,\n",
      "                               'totalStartedTwoPhaseCommit': 0},\n",
      " 'uptime': 331198.0,\n",
      " 'uptimeEstimate': 331197,\n",
      " 'uptimeMillis': 331197981,\n",
      " 'version': '4.2.2',\n",
      " 'wiredTiger': {'async': {'current work queue length': 0,\n",
      "                          'maximum work queue length': 0,\n",
      "                          'number of allocation state races': 0,\n",
      "                          'number of flush calls': 0,\n",
      "                          'number of operation slots viewed for allocation': 0,\n",
      "                          'number of times operation allocation failed': 0,\n",
      "                          'number of times worker found no work': 0,\n",
      "                          'total allocations': 0,\n",
      "                          'total compact calls': 0,\n",
      "                          'total insert calls': 0,\n",
      "                          'total remove calls': 0,\n",
      "                          'total search calls': 0,\n",
      "                          'total update calls': 0},\n",
      "                'block-manager': {'blocks pre-loaded': 0,\n",
      "                                  'blocks read': 138,\n",
      "                                  'blocks written': 717,\n",
      "                                  'bytes read': 565248,\n",
      "                                  'bytes written': 6320128,\n",
      "                                  'bytes written for checkpoint': 6320128,\n",
      "                                  'mapped blocks read': 0,\n",
      "                                  'mapped bytes read': 0},\n",
      "                'cache': {'application threads page read from disk to cache count': 0,\n",
      "                          'application threads page read from disk to cache time (usecs)': 0,\n",
      "                          'application threads page write from cache to disk count': 403,\n",
      "                          'application threads page write from cache to disk time (usecs)': 101298,\n",
      "                          'bytes belonging to page images in the cache': 43,\n",
      "                          'bytes belonging to the cache overflow table in the cache': 182,\n",
      "                          'bytes currently in the cache': 5711222,\n",
      "                          'bytes dirty in the cache cumulative': 20432072,\n",
      "                          'bytes not belonging to page images in the cache': 5711179,\n",
      "                          'bytes read into cache': 0,\n",
      "                          'bytes written from cache': 13445753,\n",
      "                          'cache overflow cursor application thread wait time (usecs)': 0,\n",
      "                          'cache overflow cursor internal thread wait time (usecs)': 0,\n",
      "                          'cache overflow score': 0,\n",
      "                          'cache overflow table entries': 0,\n",
      "                          'cache overflow table insert calls': 0,\n",
      "                          'cache overflow table max on-disk size': 0,\n",
      "                          'cache overflow table on-disk size': 0,\n",
      "                          'cache overflow table remove calls': 0,\n",
      "                          'checkpoint blocked page eviction': 0,\n",
      "                          'eviction calls to get a page': 14724,\n",
      "                          'eviction calls to get a page found queue empty': 14722,\n",
      "                          'eviction calls to get a page found queue empty after locking': 0,\n",
      "                          'eviction currently operating in aggressive mode': 0,\n",
      "                          'eviction empty score': 0,\n",
      "                          'eviction passes of a file': 0,\n",
      "                          'eviction server candidate queue empty when topping up': 0,\n",
      "                          'eviction server candidate queue not empty when topping up': 0,\n",
      "                          'eviction server evicting pages': 0,\n",
      "                          'eviction server slept, because we did not make progress with eviction': 251,\n",
      "                          'eviction server unable to reach eviction goal': 0,\n",
      "                          'eviction server waiting for a leaf page': 4,\n",
      "                          'eviction state': 128,\n",
      "                          'eviction walk target pages histogram - 0-9': 0,\n",
      "                          'eviction walk target pages histogram - 10-31': 0,\n",
      "                          'eviction walk target pages histogram - 128 and higher': 0,\n",
      "                          'eviction walk target pages histogram - 32-63': 0,\n",
      "                          'eviction walk target pages histogram - 64-128': 0,\n",
      "                          'eviction walk target strategy both clean and dirty pages': 0,\n",
      "                          'eviction walk target strategy only clean pages': 0,\n",
      "                          'eviction walk target strategy only dirty pages': 0,\n",
      "                          'eviction walks abandoned': 0,\n",
      "                          'eviction walks gave up because they restarted their walk twice': 0,\n",
      "                          'eviction walks gave up because they saw too many pages and found no candidates': 0,\n",
      "                          'eviction walks gave up because they saw too many pages and found too few candidates': 0,\n",
      "                          'eviction walks reached end of tree': 0,\n",
      "                          'eviction walks started from root of tree': 0,\n",
      "                          'eviction walks started from saved location in tree': 0,\n",
      "                          'eviction worker thread active': 4,\n",
      "                          'eviction worker thread created': 0,\n",
      "                          'eviction worker thread evicting pages': 4,\n",
      "                          'eviction worker thread removed': 0,\n",
      "                          'eviction worker thread stable number': 0,\n",
      "                          'files with active eviction walks': 0,\n",
      "                          'files with new eviction walks started': 0,\n",
      "                          'force re-tuning of eviction workers once in a while': 0,\n",
      "                          'forced eviction - pages evicted that were clean count': 0,\n",
      "                          'forced eviction - pages evicted that were clean time (usecs)': 0,\n",
      "                          'forced eviction - pages evicted that were dirty count': 3,\n",
      "                          'forced eviction - pages evicted that were dirty time (usecs)': 155,\n",
      "                          'forced eviction - pages selected because of too many deleted items count': 7,\n",
      "                          'forced eviction - pages selected count': 3,\n",
      "                          'forced eviction - pages selected unable to be evicted count': 0,\n",
      "                          'forced eviction - pages selected unable to be evicted time': 0,\n",
      "                          'hazard pointer blocked page eviction': 0,\n",
      "                          'hazard pointer check calls': 7,\n",
      "                          'hazard pointer check entries walked': 0,\n",
      "                          'hazard pointer maximum array length': 0,\n",
      "                          'in-memory page passed criteria to be split': 0,\n",
      "                          'in-memory page splits': 0,\n",
      "                          'internal pages evicted': 0,\n",
      "                          'internal pages queued for eviction': 0,\n",
      "                          'internal pages seen by eviction walk': 0,\n",
      "                          'internal pages seen by eviction walk that are already queued': 0,\n",
      "                          'internal pages split during eviction': 0,\n",
      "                          'leaf pages split during eviction': 0,\n",
      "                          'maximum bytes configured': 8053063680.0,\n",
      "                          'maximum page size at eviction': 0,\n",
      "                          'modified pages evicted': 9,\n",
      "                          'modified pages evicted by application threads': 0,\n",
      "                          'operations timed out waiting for space in cache': 0,\n",
      "                          'overflow pages read into cache': 0,\n",
      "                          'page split during eviction deepened the tree': 0,\n",
      "                          'page written requiring cache overflow records': 0,\n",
      "                          'pages currently held in the cache': 25,\n",
      "                          'pages evicted by application threads': 0,\n",
      "                          'pages queued for eviction': 0,\n",
      "                          'pages queued for eviction post lru sorting': 0,\n",
      "                          'pages queued for urgent eviction': 7,\n",
      "                          'pages queued for urgent eviction during walk': 0,\n",
      "                          'pages read into cache': 0,\n",
      "                          'pages read into cache after truncate': 16,\n",
      "                          'pages read into cache after truncate in prepare state': 0,\n",
      "                          'pages read into cache requiring cache overflow entries': 0,\n",
      "                          'pages read into cache requiring cache overflow for checkpoint': 0,\n",
      "                          'pages read into cache skipping older cache overflow entries': 0,\n",
      "                          'pages read into cache with skipped cache overflow entries needed later': 0,\n",
      "                          'pages read into cache with skipped cache overflow entries needed later by checkpoint': 0,\n",
      "                          'pages requested from the cache': 35527,\n",
      "                          'pages seen by eviction walk': 0,\n",
      "                          'pages seen by eviction walk that are already queued': 0,\n",
      "                          'pages selected for eviction unable to be evicted': 0,\n",
      "                          'pages selected for eviction unable to be evicted as the parent page has overflow items': 0,\n",
      "                          'pages selected for eviction unable to be evicted because of active children on an internal page': 0,\n",
      "                          'pages selected for eviction unable to be evicted because of failure in reconciliation': 0,\n",
      "                          'pages selected for eviction unable to be evicted due to newer modifications on a clean page': 0,\n",
      "                          'pages walked for eviction': 0,\n",
      "                          'pages written from cache': 403,\n",
      "                          'pages written requiring in-memory restoration': 3,\n",
      "                          'percentage overhead': 8,\n",
      "                          'tracked bytes belonging to internal pages in the cache': 6037,\n",
      "                          'tracked bytes belonging to leaf pages in the cache': 5705185,\n",
      "                          'tracked dirty bytes in the cache': 0,\n",
      "                          'tracked dirty pages in the cache': 0,\n",
      "                          'unmodified pages evicted': 0},\n",
      "                'capacity': {'background fsync file handles considered': 0,\n",
      "                             'background fsync file handles synced': 0,\n",
      "                             'background fsync time (msecs)': 0,\n",
      "                             'bytes read': 0,\n",
      "                             'bytes written for checkpoint': 3862115,\n",
      "                             'bytes written for eviction': 0,\n",
      "                             'bytes written for log': 2329600,\n",
      "                             'bytes written total': 6191715,\n",
      "                             'threshold to call fsync': 0,\n",
      "                             'time waiting due to total capacity (usecs)': 0,\n",
      "                             'time waiting during checkpoint (usecs)': 0,\n",
      "                             'time waiting during eviction (usecs)': 0,\n",
      "                             'time waiting during logging (usecs)': 0,\n",
      "                             'time waiting during read (usecs)': 0},\n",
      "                'concurrentTransactions': {'read': {'available': 127,\n",
      "                                                    'out': 1,\n",
      "                                                    'totalTickets': 128},\n",
      "                                           'write': {'available': 128,\n",
      "                                                     'out': 0,\n",
      "                                                     'totalTickets': 128}},\n",
      "                'connection': {'auto adjusting condition resets': 484,\n",
      "                               'auto adjusting condition wait calls': 666496,\n",
      "                               'detected system time went backwards': 0,\n",
      "                               'files currently open': 16,\n",
      "                               'memory allocations': 2538107,\n",
      "                               'memory frees': 2521440,\n",
      "                               'memory re-allocations': 467312,\n",
      "                               'pthread mutex condition wait calls': 1461567,\n",
      "                               'pthread mutex shared lock read-lock calls': 1203163,\n",
      "                               'pthread mutex shared lock write-lock calls': 111657,\n",
      "                               'total fsync I/Os': 454,\n",
      "                               'total read I/Os': 219,\n",
      "                               'total write I/Os': 952},\n",
      "                'cursor': {'cached cursor count': 12,\n",
      "                           'cursor bulk loaded cursor insert calls': 0,\n",
      "                           'cursor close calls that result in cache': 8720,\n",
      "                           'cursor create calls': 388,\n",
      "                           'cursor insert calls': 9073,\n",
      "                           'cursor insert key and value bytes': 8538123,\n",
      "                           'cursor modify calls': 0,\n",
      "                           'cursor modify key and value bytes affected': 0,\n",
      "                           'cursor modify value bytes modified': 0,\n",
      "                           'cursor next calls': 30462,\n",
      "                           'cursor operation restarted': 0,\n",
      "                           'cursor prev calls': 1740,\n",
      "                           'cursor remove calls': 4397,\n",
      "                           'cursor remove key bytes removed': 35570,\n",
      "                           'cursor reserve calls': 0,\n",
      "                           'cursor reset calls': 38121,\n",
      "                           'cursor search calls': 15523,\n",
      "                           'cursor search near calls': 6344,\n",
      "                           'cursor sweep buckets': 23089,\n",
      "                           'cursor sweep cursors closed': 1,\n",
      "                           'cursor sweep cursors examined': 240,\n",
      "                           'cursor sweeps': 3848,\n",
      "                           'cursor truncate calls': 0,\n",
      "                           'cursor update calls': 0,\n",
      "                           'cursor update key and value bytes': 0,\n",
      "                           'cursor update value size change': 0,\n",
      "                           'cursors reused from cache': 8401,\n",
      "                           'open cursor count': 22},\n",
      "                'data-handle': {'connection data handle size': 456,\n",
      "                                'connection data handles currently active': 24,\n",
      "                                'connection sweep candidate became referenced': 0,\n",
      "                                'connection sweep dhandles closed': 1,\n",
      "                                'connection sweep dhandles removed from hash list': 97,\n",
      "                                'connection sweep time-of-death sets': 8440,\n",
      "                                'connection sweeps': 11378,\n",
      "                                'session dhandles swept': 99,\n",
      "                                'session sweep attempts': 218},\n",
      "                'lock': {'checkpoint lock acquisitions': 1942,\n",
      "                         'checkpoint lock application thread wait time (usecs)': 41,\n",
      "                         'checkpoint lock internal thread wait time (usecs)': 0,\n",
      "                         'dhandle lock application thread time waiting (usecs)': 0,\n",
      "                         'dhandle lock internal thread time waiting (usecs)': 1,\n",
      "                         'dhandle read lock acquisitions': 444181,\n",
      "                         'dhandle write lock acquisitions': 219,\n",
      "                         'durable timestamp queue lock application thread time waiting (usecs)': 0,\n",
      "                         'durable timestamp queue lock internal thread time waiting (usecs)': 0,\n",
      "                         'durable timestamp queue read lock acquisitions': 0,\n",
      "                         'durable timestamp queue write lock acquisitions': 0,\n",
      "                         'metadata lock acquisitions': 61,\n",
      "                         'metadata lock application thread wait time (usecs)': 0,\n",
      "                         'metadata lock internal thread wait time (usecs)': 0,\n",
      "                         'read timestamp queue lock application thread time waiting (usecs)': 0,\n",
      "                         'read timestamp queue lock internal thread time waiting (usecs)': 0,\n",
      "                         'read timestamp queue read lock acquisitions': 0,\n",
      "                         'read timestamp queue write lock acquisitions': 0,\n",
      "                         'schema lock acquisitions': 80,\n",
      "                         'schema lock application thread wait time (usecs)': 3,\n",
      "                         'schema lock internal thread wait time (usecs)': 5,\n",
      "                         'table lock application thread time waiting for the table lock (usecs)': 0,\n",
      "                         'table lock internal thread time waiting for the table lock (usecs)': 0,\n",
      "                         'table read lock acquisitions': 0,\n",
      "                         'table write lock acquisitions': 15,\n",
      "                         'txn global lock application thread time waiting (usecs)': 0,\n",
      "                         'txn global lock internal thread time waiting (usecs)': 0,\n",
      "                         'txn global read lock acquisitions': 387,\n",
      "                         'txn global write lock acquisitions': 212},\n",
      "                'log': {'busy returns attempting to switch slots': 0,\n",
      "                        'force archive time sleeping (usecs)': 0,\n",
      "                        'log bytes of payload data': 2110946,\n",
      "                        'log bytes written': 2329472,\n",
      "                        'log files manually zero-filled': 0,\n",
      "                        'log flush operations': 722591,\n",
      "                        'log force write operations': 833886,\n",
      "                        'log force write operations skipped': 833825,\n",
      "                        'log records compressed': 132,\n",
      "                        'log records not compressed': 43,\n",
      "                        'log records too small to compress': 2359,\n",
      "                        'log release advances write LSN': 76,\n",
      "                        'log scan operations': 0,\n",
      "                        'log scan records requiring two reads': 0,\n",
      "                        'log server thread advances write LSN': 79,\n",
      "                        'log server thread write LSN walk skipped': 113960,\n",
      "                        'log sync operations': 142,\n",
      "                        'log sync time duration (usecs)': 864278,\n",
      "                        'log sync_dir operations': 1,\n",
      "                        'log sync_dir time duration (usecs)': 6987,\n",
      "                        'log write operations': 2534,\n",
      "                        'logging bytes consolidated': 2328960,\n",
      "                        'maximum log file size': 104857600,\n",
      "                        'number of pre-allocated log files to create': 2,\n",
      "                        'pre-allocated log files not ready and missed': 1,\n",
      "                        'pre-allocated log files prepared': 2,\n",
      "                        'pre-allocated log files used': 0,\n",
      "                        'records processed by log scan': 0,\n",
      "                        'slot close lost race': 0,\n",
      "                        'slot close unbuffered waits': 0,\n",
      "                        'slot closures': 155,\n",
      "                        'slot join atomic update races': 0,\n",
      "                        'slot join calls atomic updates raced': 0,\n",
      "                        'slot join calls did not yield': 2534,\n",
      "                        'slot join calls found active slot closed': 0,\n",
      "                        'slot join calls slept': 0,\n",
      "                        'slot join calls yielded': 0,\n",
      "                        'slot join found active slot closed': 0,\n",
      "                        'slot joins yield time (usecs)': 0,\n",
      "                        'slot transitions unable to find free slot': 0,\n",
      "                        'slot unbuffered writes': 0,\n",
      "                        'total in-memory size of compressed records': 8571786,\n",
      "                        'total log buffer size': 33554432,\n",
      "                        'total size of compressed records': 2006534,\n",
      "                        'written slots coalesced': 0,\n",
      "                        'yields waiting for previous log file close': 0},\n",
      "                'perf': {'file system read latency histogram (bucket 1) - 10-49ms': 0,\n",
      "                         'file system read latency histogram (bucket 2) - 50-99ms': 0,\n",
      "                         'file system read latency histogram (bucket 3) - 100-249ms': 0,\n",
      "                         'file system read latency histogram (bucket 4) - 250-499ms': 0,\n",
      "                         'file system read latency histogram (bucket 5) - 500-999ms': 0,\n",
      "                         'file system read latency histogram (bucket 6) - 1000ms+': 0,\n",
      "                         'file system write latency histogram (bucket 1) - 10-49ms': 0,\n",
      "                         'file system write latency histogram (bucket 2) - 50-99ms': 0,\n",
      "                         'file system write latency histogram (bucket 3) - 100-249ms': 0,\n",
      "                         'file system write latency histogram (bucket 4) - 250-499ms': 0,\n",
      "                         'file system write latency histogram (bucket 5) - 500-999ms': 0,\n",
      "                         'file system write latency histogram (bucket 6) - 1000ms+': 0,\n",
      "                         'operation read latency histogram (bucket 1) - 100-249us': 0,\n",
      "                         'operation read latency histogram (bucket 2) - 250-499us': 0,\n",
      "                         'operation read latency histogram (bucket 3) - 500-999us': 0,\n",
      "                         'operation read latency histogram (bucket 4) - 1000-9999us': 2,\n",
      "                         'operation read latency histogram (bucket 5) - 10000us+': 0,\n",
      "                         'operation write latency histogram (bucket 1) - 100-249us': 2,\n",
      "                         'operation write latency histogram (bucket 2) - 250-499us': 1,\n",
      "                         'operation write latency histogram (bucket 3) - 500-999us': 0,\n",
      "                         'operation write latency histogram (bucket 4) - 1000-9999us': 0,\n",
      "                         'operation write latency histogram (bucket 5) - 10000us+': 0},\n",
      "                'reconciliation': {'fast-path pages deleted': 0,\n",
      "                                   'page reconciliation calls': 332,\n",
      "                                   'page reconciliation calls for eviction': 6,\n",
      "                                   'pages deleted': 32,\n",
      "                                   'split bytes currently awaiting free': 0,\n",
      "                                   'split objects currently awaiting free': 0},\n",
      "                'session': {'open session count': 19,\n",
      "                            'session query timestamp calls': 0,\n",
      "                            'table alter failed calls': 0,\n",
      "                            'table alter successful calls': 0,\n",
      "                            'table alter unchanged and skipped': 0,\n",
      "                            'table compact failed calls': 0,\n",
      "                            'table compact successful calls': 0,\n",
      "                            'table create failed calls': 0,\n",
      "                            'table create successful calls': 12,\n",
      "                            'table drop failed calls': 0,\n",
      "                            'table drop successful calls': 1,\n",
      "                            'table import failed calls': 0,\n",
      "                            'table import successful calls': 0,\n",
      "                            'table rebalance failed calls': 0,\n",
      "                            'table rebalance successful calls': 0,\n",
      "                            'table rename failed calls': 0,\n",
      "                            'table rename successful calls': 0,\n",
      "                            'table salvage failed calls': 0,\n",
      "                            'table salvage successful calls': 0,\n",
      "                            'table truncate failed calls': 0,\n",
      "                            'table truncate successful calls': 0,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            'table verify failed calls': 0,\n",
      "                            'table verify successful calls': 0},\n",
      "                'snapshot-window-settings': {'cache pressure percentage threshold': 95,\n",
      "                                             'current available snapshots window size in seconds': 0,\n",
      "                                             'current cache pressure percentage': 0,\n",
      "                                             'latest majority snapshot timestamp available': 'Dec '\n",
      "                                                                                             '31 '\n",
      "                                                                                             '19:00:00:0',\n",
      "                                             'max target available snapshots window size in seconds': 5,\n",
      "                                             'oldest majority snapshot timestamp available': 'Dec '\n",
      "                                                                                             '31 '\n",
      "                                                                                             '19:00:00:0',\n",
      "                                             'target available snapshots window size in seconds': 5,\n",
      "                                             'total number of SnapshotTooOld errors': 0},\n",
      "                'thread-state': {'active filesystem fsync calls': 0,\n",
      "                                 'active filesystem read calls': 0,\n",
      "                                 'active filesystem write calls': 0},\n",
      "                'thread-yield': {'application thread time evicting (usecs)': 0,\n",
      "                                 'application thread time waiting for cache (usecs)': 0,\n",
      "                                 'connection close blocked waiting for transaction state stabilization': 0,\n",
      "                                 'connection close yielded for lsm manager shutdown': 0,\n",
      "                                 'data handle lock yielded': 0,\n",
      "                                 'get reference for page index and slot time sleeping (usecs)': 0,\n",
      "                                 'log server sync yielded for log write': 0,\n",
      "                                 'page access yielded due to prepare state change': 0,\n",
      "                                 'page acquire busy blocked': 0,\n",
      "                                 'page acquire eviction blocked': 0,\n",
      "                                 'page acquire locked blocked': 0,\n",
      "                                 'page acquire read blocked': 0,\n",
      "                                 'page acquire time sleeping (usecs)': 0,\n",
      "                                 'page delete rollback time sleeping for state change (usecs)': 0,\n",
      "                                 'page reconciliation yielded due to child modification': 0},\n",
      "                'transaction': {'Number of prepared updates': 0,\n",
      "                                'Number of prepared updates added to cache overflow': 0,\n",
      "                                'durable timestamp queue entries walked': 0,\n",
      "                                'durable timestamp queue insert to empty': 0,\n",
      "                                'durable timestamp queue inserts to head': 0,\n",
      "                                'durable timestamp queue inserts total': 0,\n",
      "                                'durable timestamp queue length': 0,\n",
      "                                'number of named snapshots created': 0,\n",
      "                                'number of named snapshots dropped': 0,\n",
      "                                'prepared transactions': 0,\n",
      "                                'prepared transactions committed': 0,\n",
      "                                'prepared transactions currently active': 0,\n",
      "                                'prepared transactions rolled back': 0,\n",
      "                                'query timestamp calls': 111074,\n",
      "                                'read timestamp queue entries walked': 0,\n",
      "                                'read timestamp queue insert to empty': 0,\n",
      "                                'read timestamp queue inserts to head': 0,\n",
      "                                'read timestamp queue inserts total': 0,\n",
      "                                'read timestamp queue length': 0,\n",
      "                                'rollback to stable calls': 0,\n",
      "                                'rollback to stable updates aborted': 0,\n",
      "                                'rollback to stable updates removed from cache overflow': 0,\n",
      "                                'set timestamp calls': 0,\n",
      "                                'set timestamp durable calls': 0,\n",
      "                                'set timestamp durable updates': 0,\n",
      "                                'set timestamp oldest calls': 0,\n",
      "                                'set timestamp oldest updates': 0,\n",
      "                                'set timestamp stable calls': 0,\n",
      "                                'set timestamp stable updates': 0,\n",
      "                                'transaction begins': 9143,\n",
      "                                'transaction checkpoint currently running': 0,\n",
      "                                'transaction checkpoint generation': 62,\n",
      "                                'transaction checkpoint max time (msecs)': 145,\n",
      "                                'transaction checkpoint min time (msecs)': 15,\n",
      "                                'transaction checkpoint most recent time (msecs)': 15,\n",
      "                                'transaction checkpoint scrub dirty target': 0,\n",
      "                                'transaction checkpoint scrub time (msecs)': 0,\n",
      "                                'transaction checkpoint total time (msecs)': 2072,\n",
      "                                'transaction checkpoints': 1942,\n",
      "                                'transaction checkpoints skipped because database was clean': 1881,\n",
      "                                'transaction failures due to cache overflow': 0,\n",
      "                                'transaction fsync calls for checkpoint after allocating the transaction ID': 61,\n",
      "                                'transaction fsync duration for checkpoint after allocating the transaction ID (usecs)': 0,\n",
      "                                'transaction range of IDs currently pinned': 0,\n",
      "                                'transaction range of IDs currently pinned by a checkpoint': 0,\n",
      "                                'transaction range of IDs currently pinned by named snapshots': 0,\n",
      "                                'transaction range of timestamps currently pinned': 0,\n",
      "                                'transaction range of timestamps pinned by a checkpoint': 0,\n",
      "                                'transaction range of timestamps pinned by the oldest active read timestamp': 0,\n",
      "                                'transaction range of timestamps pinned by the oldest timestamp': 0,\n",
      "                                'transaction read timestamp of the oldest active reader': 0,\n",
      "                                'transaction sync calls': 0,\n",
      "                                'transactions committed': 2317,\n",
      "                                'transactions rolled back': 6876,\n",
      "                                'update conflicts': 0},\n",
      "                'uri': 'statistics:'}}\n",
      "['admin', 'config', 'energy_data', 'local']\n"
     ]
    }
   ],
   "source": [
    "import pymongo\n",
    "from pprint import pprint\n",
    "\n",
    "client = pymongo.MongoClient('mongodb://localhost/')\n",
    "db = client.admin\n",
    "\n",
    "# Issue the serverStatus command and print the results\n",
    "serverStatusResult=db.command(\"serverStatus\")\n",
    "pprint(serverStatusResult)\n",
    "\n",
    "mydb = client['energy_data']\n",
    "\n",
    "print(client.list_database_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "energy_collection = mydb['energy_data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_results = energy_collection.insert_many(environmental_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_gdp_results = energy_collection.insert_many(pop_gdp_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_results = energy_collection.insert_many(processed_temp_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice querying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "energy = [x for x in energy_collection.find({})]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: remove is deprecated. Use delete_one or delete_many instead.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'n': 2080, 'ok': 1.0}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "energy_collection.remove({'sector': {'$in':sectors}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2234"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(energy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 52 series with less than 58 data points\n",
      "of those, 52 of them are gdp\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "gdp_count = 0\n",
    "for series in all_states:\n",
    "    if len(series.get('data')) < 58:\n",
    "        count+=1\n",
    "        if series.get('description') == 'GDP':\n",
    "            gdp_count += 1\n",
    "        \n",
    "        \n",
    "        \n",
    "#         print(series)\n",
    "    \n",
    "print(f'there are {count} series with less than 58 data points')\n",
    "print(f'of those, {gdp_count} of them are gdp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did some digging and turns out GDP only goes back to 1997 because that was when data transitioned from Standard Industrial Classification (SIC) to North American Industry Classification System (NAICS). This gives us only about 20 years of data to work with for GDP. Since our data is in annual increments, we only have roughly 20 data points which will not be enough for time series analysis (usually sounds like 50-60 points minimum). Thus, we will exclude GDP from the exogenous vars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['2017', 2175],\n",
       " ['2016', 1983],\n",
       " ['2015', 1787],\n",
       " ['2014', 1679],\n",
       " ['2013', 1562],\n",
       " ['2012', 1475],\n",
       " ['2011', 1377],\n",
       " ['2010', 1282],\n",
       " ['2009', 1211],\n",
       " ['2008', 1150],\n",
       " ['2007', 1035],\n",
       " ['2006', 866],\n",
       " ['2005', 738],\n",
       " ['2004', 678],\n",
       " ['2003', 674],\n",
       " ['2002', 659],\n",
       " ['2001', 652],\n",
       " ['2000', 631],\n",
       " ['1999', 645],\n",
       " ['1998', 621],\n",
       " ['1997', 599],\n",
       " ['1996', 571],\n",
       " ['1995', 514],\n",
       " ['1994', 463],\n",
       " ['1993', 413],\n",
       " ['1992', 377],\n",
       " ['1991', 352],\n",
       " ['1990', 314],\n",
       " ['1989', 282],\n",
       " ['1988', 0],\n",
       " ['1987', 0],\n",
       " ['1986', 0],\n",
       " ['1985', 0],\n",
       " ['1984', 0],\n",
       " ['1983', 0],\n",
       " ['1982', 0],\n",
       " ['1981', 0],\n",
       " ['1980', 0],\n",
       " ['1979', 0],\n",
       " ['1978', 0],\n",
       " ['1977', 0],\n",
       " ['1976', 0],\n",
       " ['1975', 0],\n",
       " ['1974', 0],\n",
       " ['1973', 0],\n",
       " ['1972', 0],\n",
       " ['1971', 0],\n",
       " ['1970', 0],\n",
       " ['1969', 0],\n",
       " ['1968', 0],\n",
       " ['1967', 0],\n",
       " ['1966', 0],\n",
       " ['1965', 0],\n",
       " ['1964', 0],\n",
       " ['1963', 0],\n",
       " ['1962', 0],\n",
       " ['1961', 0],\n",
       " ['1960', 0]]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "short_list['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Natural Gas including Supplemental Gaseous Fuels\n",
      "Solar energy\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Length of passed values is 58, index implies 59",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-442d7d65120d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseries\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'energy_type'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mts_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtuple_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtuple_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseries\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mts_values\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseries\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'energy_type'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, dtype, name, copy, fastpath)\u001b[0m\n\u001b[1;32m    300\u001b[0m                         raise ValueError(\n\u001b[1;32m    301\u001b[0m                             \u001b[0;34m\"Length of passed values is {val}, \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m                             \u001b[0;34m\"index implies {ind}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m                         )\n\u001b[1;32m    304\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Length of passed values is 58, index implies 59"
     ]
    }
   ],
   "source": [
    "dates = np.arange(2018,1959,-1)\n",
    "df = pd.DataFrame(index = dates)\n",
    "\n",
    "for series in oregon:\n",
    "    if series.get('sector') == 'Residential Sector':\n",
    "\n",
    "        ts_values = [tuple_[1] for tuple_ in series['data']]\n",
    "        df = pd.concat([df, pd.Series(data = ts_values,name=series['energy_type'], index=dates)],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Natural Gas including Supplemental Gaseous Fuels</th>\n",
       "      <th>Solar energy</th>\n",
       "      <th>Geothermal</th>\n",
       "      <th>Coal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018</th>\n",
       "      <td>30969</td>\n",
       "      <td>30969</td>\n",
       "      <td>30969</td>\n",
       "      <td>30969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017</th>\n",
       "      <td>33987</td>\n",
       "      <td>33987</td>\n",
       "      <td>33987</td>\n",
       "      <td>33987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016</th>\n",
       "      <td>28570</td>\n",
       "      <td>28570</td>\n",
       "      <td>28570</td>\n",
       "      <td>28570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015</th>\n",
       "      <td>26962</td>\n",
       "      <td>26962</td>\n",
       "      <td>26962</td>\n",
       "      <td>26962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014</th>\n",
       "      <td>29185</td>\n",
       "      <td>29185</td>\n",
       "      <td>29185</td>\n",
       "      <td>29185</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Natural Gas including Supplemental Gaseous Fuels  Solar energy  \\\n",
       "2018                                             30969         30969   \n",
       "2017                                             33987         33987   \n",
       "2016                                             28570         28570   \n",
       "2015                                             26962         26962   \n",
       "2014                                             29185         29185   \n",
       "\n",
       "      Geothermal   Coal  \n",
       "2018       30969  30969  \n",
       "2017       33987  33987  \n",
       "2016       28570  28570  \n",
       "2015       26962  26962  \n",
       "2014       29185  29185  "
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['_id', 'series_id', 'sector', 'data', 'state', 'units', 'energy_type', 'renewable'])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "series.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.concat([or_df, pd.Series(values,name='test',index=dates)],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test</th>\n",
       "      <th>test 2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018</th>\n",
       "      <td>30969</td>\n",
       "      <td>30969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017</th>\n",
       "      <td>33987</td>\n",
       "      <td>33987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016</th>\n",
       "      <td>28570</td>\n",
       "      <td>28570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015</th>\n",
       "      <td>26962</td>\n",
       "      <td>26962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014</th>\n",
       "      <td>29185</td>\n",
       "      <td>29185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013</th>\n",
       "      <td>30832</td>\n",
       "      <td>30832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012</th>\n",
       "      <td>29451</td>\n",
       "      <td>29451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011</th>\n",
       "      <td>31034</td>\n",
       "      <td>31034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010</th>\n",
       "      <td>27461</td>\n",
       "      <td>27461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009</th>\n",
       "      <td>30504</td>\n",
       "      <td>30504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008</th>\n",
       "      <td>31202</td>\n",
       "      <td>31202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007</th>\n",
       "      <td>29960</td>\n",
       "      <td>29960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006</th>\n",
       "      <td>28840</td>\n",
       "      <td>28840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005</th>\n",
       "      <td>28627</td>\n",
       "      <td>28627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004</th>\n",
       "      <td>26438</td>\n",
       "      <td>26438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2003</th>\n",
       "      <td>26294</td>\n",
       "      <td>26294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2002</th>\n",
       "      <td>28417</td>\n",
       "      <td>28417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2001</th>\n",
       "      <td>28690</td>\n",
       "      <td>28690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000</th>\n",
       "      <td>29481</td>\n",
       "      <td>29481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>30229</td>\n",
       "      <td>30229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>27285</td>\n",
       "      <td>27285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>26751</td>\n",
       "      <td>26751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>26712</td>\n",
       "      <td>26712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>23417</td>\n",
       "      <td>23417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1994</th>\n",
       "      <td>24024</td>\n",
       "      <td>24024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1993</th>\n",
       "      <td>25023</td>\n",
       "      <td>25023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1992</th>\n",
       "      <td>20318</td>\n",
       "      <td>20318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1991</th>\n",
       "      <td>23014</td>\n",
       "      <td>23014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1990</th>\n",
       "      <td>20913</td>\n",
       "      <td>20913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1989</th>\n",
       "      <td>20980</td>\n",
       "      <td>20980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1988</th>\n",
       "      <td>18829</td>\n",
       "      <td>18829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1987</th>\n",
       "      <td>17186</td>\n",
       "      <td>17186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1986</th>\n",
       "      <td>17214</td>\n",
       "      <td>17214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1985</th>\n",
       "      <td>19614</td>\n",
       "      <td>19614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1984</th>\n",
       "      <td>17624</td>\n",
       "      <td>17624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1983</th>\n",
       "      <td>15764</td>\n",
       "      <td>15764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1982</th>\n",
       "      <td>17049</td>\n",
       "      <td>17049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1981</th>\n",
       "      <td>15578</td>\n",
       "      <td>15578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1980</th>\n",
       "      <td>15872</td>\n",
       "      <td>15872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1979</th>\n",
       "      <td>14439</td>\n",
       "      <td>14439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1978</th>\n",
       "      <td>12671</td>\n",
       "      <td>12671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1977</th>\n",
       "      <td>11317</td>\n",
       "      <td>11317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1976</th>\n",
       "      <td>14499</td>\n",
       "      <td>14499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1975</th>\n",
       "      <td>16516</td>\n",
       "      <td>16516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1974</th>\n",
       "      <td>13991</td>\n",
       "      <td>13991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1973</th>\n",
       "      <td>14541</td>\n",
       "      <td>14541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1972</th>\n",
       "      <td>15184</td>\n",
       "      <td>15184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1971</th>\n",
       "      <td>14173</td>\n",
       "      <td>14173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1970</th>\n",
       "      <td>11871</td>\n",
       "      <td>11871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1969</th>\n",
       "      <td>10503</td>\n",
       "      <td>10503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1968</th>\n",
       "      <td>8299</td>\n",
       "      <td>8299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1967</th>\n",
       "      <td>7372</td>\n",
       "      <td>7372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1966</th>\n",
       "      <td>7005</td>\n",
       "      <td>7005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1965</th>\n",
       "      <td>5983</td>\n",
       "      <td>5983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1964</th>\n",
       "      <td>5708</td>\n",
       "      <td>5708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1963</th>\n",
       "      <td>4505</td>\n",
       "      <td>4505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1962</th>\n",
       "      <td>3687</td>\n",
       "      <td>3687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1961</th>\n",
       "      <td>3373</td>\n",
       "      <td>3373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1960</th>\n",
       "      <td>3215</td>\n",
       "      <td>3215</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       test  test 2\n",
       "2018  30969   30969\n",
       "2017  33987   33987\n",
       "2016  28570   28570\n",
       "2015  26962   26962\n",
       "2014  29185   29185\n",
       "2013  30832   30832\n",
       "2012  29451   29451\n",
       "2011  31034   31034\n",
       "2010  27461   27461\n",
       "2009  30504   30504\n",
       "2008  31202   31202\n",
       "2007  29960   29960\n",
       "2006  28840   28840\n",
       "2005  28627   28627\n",
       "2004  26438   26438\n",
       "2003  26294   26294\n",
       "2002  28417   28417\n",
       "2001  28690   28690\n",
       "2000  29481   29481\n",
       "1999  30229   30229\n",
       "1998  27285   27285\n",
       "1997  26751   26751\n",
       "1996  26712   26712\n",
       "1995  23417   23417\n",
       "1994  24024   24024\n",
       "1993  25023   25023\n",
       "1992  20318   20318\n",
       "1991  23014   23014\n",
       "1990  20913   20913\n",
       "1989  20980   20980\n",
       "1988  18829   18829\n",
       "1987  17186   17186\n",
       "1986  17214   17214\n",
       "1985  19614   19614\n",
       "1984  17624   17624\n",
       "1983  15764   15764\n",
       "1982  17049   17049\n",
       "1981  15578   15578\n",
       "1980  15872   15872\n",
       "1979  14439   14439\n",
       "1978  12671   12671\n",
       "1977  11317   11317\n",
       "1976  14499   14499\n",
       "1975  16516   16516\n",
       "1974  13991   13991\n",
       "1973  14541   14541\n",
       "1972  15184   15184\n",
       "1971  14173   14173\n",
       "1970  11871   11871\n",
       "1969  10503   10503\n",
       "1968   8299    8299\n",
       "1967   7372    7372\n",
       "1966   7005    7005\n",
       "1965   5983    5983\n",
       "1964   5708    5708\n",
       "1963   4505    4505\n",
       "1962   3687    3687\n",
       "1961   3373    3373\n",
       "1960   3215    3215"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.concat([test_df, pd.Series(values, name='test 2',index=dates)],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "316px",
    "left": "916px",
    "right": "20px",
    "top": "120px",
    "width": "344px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
